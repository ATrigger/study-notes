#+TODO: X 0 1 2 | 3
#+TITLE: Tickets for exam on operating systems lectures course given by Yan Malakhovski, 4th term, 2015

* Legend
  X -- нифига не написано и непонятно, где брать
  0 -- нифига не написано
  1 -- что-то написано, но не дописано
  2 -- написано, но возможно, что что-то серьезно упущено
  3 -- все хорошо.
* Sources  Тут только самые важные и обширные источники. Всякие статьи, которые по мере необходимости встречаются в тексте, встречаются только там (и в заголовках тем).
  1. Raw notes by volhovm
  2. OS course plan
     http://rain.ifmo.ru/~trojan/linux/year2009/
  3. Yet another plan
     http://rain.ifmo.ru/~trojan/linux/year2007/
  4. Prev. year googledoc
     https://docs.google.com/spreadsheets/d/1CoPjN7shou3m3kAQdsRHY1HxQYyKho5f7Qn1KeL9n-U/edit#gid=0
  5. Malakhovski's notes
     https://github.com/oxij/unix-notes-ru/blob/master/compiled/main.pdf
  6. Wikipedia ofc
* 2 Ticket 1
** RAM, SRAM, DRAM
   RAM (random-access memory) - это оперативная память.
   От постоянной памяти ее отличает:
   1) скорость работы
   2) потеря данных в отсутствие питания
   3) да вообще все по-другому!

   SRAM (static random-access memory) - статическая память.
   CMOS-ячейка SRAM основана на защелке: [[http://2.bp.blogspot.com/-dCCrTGB-c6U/T1zaY5TG1oI/AAAAAAAAAu8/MutoYbjglvs/s1600/SRAM.gif][схема]]
   *Read-операция*: мы даем на WL напряжение, сигнал уходит с BL и ¬BL.
   *Write-операция*: мы даем на WL напряжение и в BL тоже даем сигнал, он запоминается.

   Pros:
   + Быстрая
   + Не надо ничего перезаписывать
   Cons:
   - Защелки жрут электричество все время
   - Схема ячейки сложная, делать дороже
   - Ячейка занимает много площади

   DRAM (dynamic random-access memory) - динамическая память.
   CMOS-ячейка DRAM основана на конденсаторе: [[https://www.cs.auckland.ac.nz/~jmor159/363/html/fig/dram_cell.gif][схема]], [[https://upload.wikimedia.org/wikipedia/commons/3/3d/Square_array_of_mosfet_cells_read.png][схема банка]]
   *Read-операция*: мы даем на WL напряжение, конденсаторы разряжаются, сигнал уходит,
   и теперь его надо перезаписать обратно.
   *Write-операция*: даем на WL и BLs напряжения, те конденсаторы, на которых 1, заряжаются,
   а те, на которых 0 - разряжаются.
   *Замечание*: так как конденсаторы разряжаются сами по себе, периодически надо их перезаписывать
   (memory refresh)

   Pros:
   + Дешевая
   + Ячейки маленькие, можно много понапихать
   + Жрет энергию только во время read/write и refresh
   Cons:
   - Медленная (конденсаторы разряжаются не мгновенно)
   - Refresh-и тоже не прибавляют отзывчивости

** Структурно-операционная схема обычной планки памяти (DDR)
   Схема одного банка: [[https://upload.wikimedia.org/wikipedia/commons/3/3d/Square_array_of_mosfet_cells_read.png][схема банка]]
   Банки лежат на планочке рядом. Верхняя часть адреса ячейки отвечает за номер банка

   DDR (double-data rate) - хитрость, позволяющая передавать 2 слова за такт процессора вместо 1
   Хитрость в том, что мы передаем данные как на восходящем, так и на нисходящем фронте меандра.
** Кеши CPU: L1, L2, L3
   Кэш - быстрый кусок SRAM рядом с процессором, в который складываются часто используемые данные. Кэш - всегда SRAM, чтобы было быстро, и потому всегда маленький, чтобы было не так дорого и энергозатратно.

   L1 - это самый маленький и близкий к процессору кэш. Он обычно сидит на том же куске кремния. Иногда подразделяется на L1i (кэш инструкций) b L1d (кэш данных). Его подстраховывает кэш L2 - который побольше и чуть подальше.
   Для многоядерных процессоров есть также кэш L3, к которому могут обращаться все ядра. (подробнее про это все - в последнем пункте)
   Кэши могут быть inclusive (L1 ⊂ L2 ⊂ L3 - данные дублируются) или exclusive (данные не дублируются).

   Кэш разбит на кэш-линии. Структура кэш-линии:
   | tag | data block | flags |
   Плюс известен номер кэш-линии.
   Мы делим:
   * tag - кусок адреса того куска памяти, который дублируется в этой кэш-линии. За тегом закреплен некоторый последовательный регион данных размера 2^(length tag). Нужен, чтобы искать данные по адресу в кэше.
   * data - собственно, сами данные (может, 256 байт, много).
   * flags - различная мета-инфа, а еще тут бывают коды коррекции ошибок.

   Обычный адрес в памяти выглядит так:
   | tag | index | offset |
   * tag - как раз тот кусок, который матчится с tag в кэше.
   * index - индекс кэш-линии, определяет, в каком наборе линий искать данные.
   * offset - отступ от начала линии.

   Контроллер обычной памяти же воспринимает адрес по-своему (как?)

   По методике синхронизации с памятью кэши бывают write-through и write-back. write-through - это когда любой запрос на запись всегда досылается в память, write-back - это когда данные из кэша дампятся в память только при вытеснении.

   sources:
   [[https://en.wikipedia.org/wiki/CPU_cache]]
   [[http://arstechnica.com/gadgets/2002/07/caching/2/]]
** Схема параллельного извлечения
   Параллельное извлечение используется в кэше (там где у кэш-линий есть tag). Мы просто посылаем tag адреса на компараторы кэш-линий. Компаратор сравнивает tag линии с переданным, и высылает 1, если он совпал, и 0 - если нет.

   Данные извлекаются из всех кэш-линий одновременно, и данные каждой кэш-линии and-ятся с результатом компаратора. Выходные провода данных спаиваются вместе, и в итоге на выходе получаются только данные из кэш-линии с нужным тэгом.

   Если же все компараторы вернули 0, то мы детектируем cache miss и перенаправляем запрос к контроллеру памяти.

   sources:
   [[http://www.csbio.unc.edu/mcmillan/Media/L20Spring2012.pdf]]
   [[http://lwn.net/Articles/252125/]]
** Извлечение демультиплексором
   Демультиплексор - это штука, которая принимает n-битное число и сигнал и дает этот сигнал на соответствующий числу выход (один из 2ⁿ).
   [[https://en.wikipedia.org/wiki/Multiplexer#/media/File:Demultiplexer_Example01.svg][Схема демультиплексора]]
   Извлекать данные демультиплексором надо так: [[http://lwn.net/images/cpumemory/cpumemory.9.png][схема извлечения]].

   Row Address Selection - демультиплексор (выбираем строку но номеру).
   Column Address Selection - мультиплексор (выбираем столбец по номеру из тех, что пришли).

   Демультиплексор также используется в кэшах с неполной ассоциативностью, где с его помощью извлекаются кэш-линии с заданным индексом.
** Ассоциативность
   Суть кэша - в том, чтобы быстро проверить, лежат ли данные по заданному адресу в кэше, и если да - вернуть их.
   Если кэш полностью ассоциативный (fully-associative), то мы должны сделать параллельное извлечение сразу из всего кэша! С этим есть несколько бед:
   - Теги должны быть большими, слишком много места в кэше отведено под тэги
   - Теги большие - и компараторы для них тоже большие, а значит - дорогие и медленные

   Однако, есть плюсы:
   + Не бывает коллизий кэша - если мы хотим положить что-то в кэш и в нем еще вообще есть место, нам это удастся
   + Как следствие этого, происходит мало cache miss-ов

   Противоположность: direct-mapped cache.
   Адрес в памяти однозначно определяет кэш-линию, в которой эта ячейка памяти может содержаться.

   Делается это так: адрес демультиплексируется по index, выбирается 1 кэш-линия,
   тэг в ней сравнивается с данным, и если тэг равен - данные кэш-линии возвращаются.
   Иначе - регистрируется кэш-мисс.

   Pros:
   + Все это делается быстро, нет кучи компараторов
   Cons:
   - Так как каждая ячейка памяти может быть сохранена только в 1 кэш-линии,
     возникает множество *коллизий* - это когда 2 разные ячейки попадают в одну
     и ту же кэш-линию и кто-то из них вытесняет другую
   - Вследствие этого, много кэш-миссов

   Золотая середина - n-ассоциативный кэш, когда каждая ячейка может содержаться в
   одной из n линий. Делается это так:
   1) Демультиплексор отсеивает n кэш-линий по index адреса
   2) Из этих n линий по тэгу извлекаются параллельно данные

   sources:
   [[http://www.csbio.unc.edu/mcmillan/Media/L20Spring2012.pdf]]
   [[http://arstechnica.com/gadgets/2002/07/caching/5/]]
   [[http://arstechnica.com/gadgets/2002/07/caching/6/]]
** TLB
   TLB - translation lookaside buffer - это такой специальный кэш, который маппит виртуальные адреса в реальные.
   Он небольшой, и, конечно, не содержит все используемые виртуальные адреса, а лишь часто используемые.
   Трансляцией из виртуальных адресов в реальные занимается MMU - memory management unit - специальный кусок процессора.
   Он глядит в TLB, и если не находит там, пускается в долгий путь по page table-ам в основной памяти
** Общее влияние кеша на работу с памятью
   Кэш, в целом, ускоряет работу с памятью (кто бы мог подумать?). Иногда получается так, что работа идет только с кэшом, а к памяти обращений и вовсе нет (в случае look-aside).
   ?? Что здесь написать ??
** Кеши в мультипроцессорных системах и когерентность кешей
   Если у нас есть много ядер, то у каждого ядра есть собственные кэши L1 и L2.
   Однако, что же делать, если одна и та же ячейка памяти продублирована в кэшах разных ядер, и одно ядро меняет эту ячейку в своем кэше?
   Другое должно как-то увидеть это изменение.

   Для таких ситуаций существует *протоколы когерентности кэша*. Например, MESI:

   Во flags каждой кэш-линии кодируется ее состояние, 1 из 4:
   * Modified  - актуальная кэш-линия есть только в этом кэше, и она была изменена, то есть не соответствует данным в основной памяти.
   * Exclusive - кэш-линия актуальна только в этом кэше, и она совпадает с данными в памяти.
   * Shared    - кэш-линия совпадает с данными в памяти и может присутствовать в нескольких кэшах.
   * Invalid   - кэш-линия невалидна.

   Read может происходить из любого состояния, кроме Invalid. Если пытаемся читать из Invalid, то нужно сначала пофетчить данные (извлечь из соседних кэшей или из памяти).
   Write может происходить только в Modified или Exclusive. Иначе сначала нам нужно инвалидировать все копии в других кэшах, а потом выставить статус Modified.
   Перед инвалидацией Modified-линии нужно сбросить данные из нее в память (write-back).

   sources:
   [[https://en.wikipedia.org/wiki/MESI_protocol]]
* 0 Ticket 2
** Пайплайн и стадии
   Исполнение инструкций - сложный процесс, включающий в себя много этапов.
   Чтобы было быстро, процессор разбивает инструкции на этапы и выполняет их на конвейере (pipeline)

   Этапы конвейера:
   1) Fetch
      Получение инструкции по адресу, на который указывает IP. Обычно достается
      из кэша L1i
   2) Decode
      Декодирует полученную инструкцию и, таким образом, определяет, что делать дальше
      (сколько аргументов фетчить, куда их посылать и так далее
   3) FetchArgs
      Получает все аргументы инструкции (в том числе, вычисляет effective address)
   4) Execute
      Непосредственно выполняет инструкцию
   5) Commit
      Записывает результаты в регистры/память

   [[file+emacs:pipeline.hs][Модель пайплайна на Хаскеле]]

   sources:
   [[https://en.wikipedia.org/wiki/Classic_RISC_pipeline]]
** Регистровый файл
   Регистровый файл - SRAM-массивчик в процессоре. Содержит в себе ячейки с регистрами.
   В простых процессорах имена регистров в коде напрямую маппятся в эти ячейки,
   в процах покруче (современных) они иногда просто переименовываются
   (так себя ведет, например, процедура XCHG (кажется))
** Пузыри (pipeline bubbles)
   Пузырь - это последовательность nop-ов. Он появляется, когда только что зафетченную инструкцию нельзя сразу начать исполнять - например, она зависит от результата предыдущей, которая еще не завершила выполнение.
   Появление пузырей - это самый простой способ решения data hazard (проблемы зависимостей данных) и control hazard (проблемы переходов)
   Другим способом решения data hazards является forwarding - отсылка полученного результата инструкции назад.

   sources:
   [[https://en.wikipedia.org/wiki/Bubble_(computing)]]
** Предсказание переходов (branch prediction)
   Когда процессор натыкается на инструкцию j* (условный переход), он должен ее распарсить и выполнить, прежде чем станет ясно, куда нужно сдвинуть IP. Однако, если простаивать все это время, пока инструкция не выполнится, будет очень долго. Поэтому процессор пытается угадать, куда все-таки в итоге нужно будет прыгнуть, и начинает фетчить и выполнять инструкции оттуда. Если же предсказание было неверно, весь пайплайн сбрасывается, и инструкции фетчатся заново с верного адреса.

   Отсутствие branch-prediction'a - это когда IP всегда просто сдвигается на единичку вперед. Чуть более продвинутые процы, кстати, обрабатывают безусловные переходы (jmp) на decode-стадии, иначе бы каждый безусловный переход был долгим и мучительным.

   Самый простой нетривиальный бранч-предиктор - это 2 бита на каждый джамп, которые олицетворяют одно из 4 состояний:
   | True | Almost true | Almost false | False |
   Если в результате выполнения условия мы все-таки прыгнули, тогда состояние предиктора сдвигается влево. Иначе - вправо.
   Понятно, что если предиктор находится в состоянии True/Almost true, то следующие инструкции мы фетчим из адреса прыжка.
   Иначе - следующие инструкции фетчатся со следующего адреса.

   В современных компуктерах branch predictor'ы гораздо более хитровыебанные, там таблицы всякие, но достаточно этого сказать, пожалуй.
** Out of order исполнение

** Интерфейс между устройствами ввода и CPU
** Прерывания
** DMA
* 0 Ticket 3
** Виртуальная память
** MMU: TLB, каталог страниц (page table)
** Биты: readable, writeable, executable, present, dirty, copy-on-write
** IOMMU
** Память процессов
** Общая память
** Пулы памяти со специальными требованиями.
* 0 Ticket 4
** "География" адресного пространства процесса
** Структуры ядра описывающие процесс с MMU: работа с физической памятью, VMA
** Системные вызовы: brk, sbrk, mmap
** Общая память: mmaping файлов и shm_open
** Реализация malloc.
* 0 Ticket 5
** Процессы и треды
** init, родители, дети, зомби
** Треды, группы тредов, процессы, группы процессов, сессии
** Системные вызовы fork, clone, exec, wait
** Интерфейс bash
** Реализация переключения контекстов процессов: структуры данных ядра, состояния процессов, различные методы реализации CPS-преобразования.
* 0 Ticket 6
** Файловые дескрипторы и пайпы
** Системные вызовы open, read, write, close
** Структуры данных ядра: таблица файловых дескрипторов, файловые объекты POSIX, флаг CLOEXEC
** Системные вызовы dup2, fcntl, flock
** Структуры данных ядра для реализации пайпов, семафоров, блокировок на файлы.
* 0 Ticket 7
** Драйвера устройств в пространстве ядра
** Прерывания
** Монолитная обработка прерываний
** Hi/Lo прерывания
** Polling
** Драйвера как контексты исполнения и их отличия от процессов
** Реализация драйверов: структуры данных ядра, различные методы реализации CPS-преобразования.
* 0 Ticket 8    FS
** Файловые системы
** Структура данных inode
** VFS
** Структуры данных: FSObject, Namespace
** Path resolution
** Операции над неймспейсами: mount, bind mount, move mount, chroot, pivot_root
** Linux FUSE
** ФС как функция inodeno → inode
** Структуры ядра: файловый дескриптор (на устройство, файл, директорию), различные кеши
** mmaping файлов.
* 0 Ticket 9    Users/access
** Пользователи и права
** Модели прав доступа к объектам: дискретная и ролевая
** Права на объекты файловой системы
** Пользователи и группы с точки зрения ядра
** Пользователи и группы с точки зрения пространства пользователя
** Системные вызовы setuid, setgid и товарищи
** setuid bit
** PAM
** /etc/passwd, /etc/shadow, /etc/group
** Capabilities.
* 0 Ticket 10   Signals
** Стандартные сигналы
** Маски
** Правила доставки
** Реалтаймовые сигналы
** Маски и очереди
** Правила доставки
** Системные вызовы kill, sigaction
** Прерывание сигналами: кода программы, обработчиков сигналов, системных вызовов
** Реентрабельность и безопасные системные вызовы
** Сигналы и треды
** Семантика сигналов: TERM, KILL, STOP, CONT, CHLD, PIPE, ILL/FPE, SEGV, BUS.
* 0 Ticket 11   Polling
** Мултиплексирование ввода-вывода
** O_NONBLOCK
** Edge и level triggered события
** Преобразование асинхронного ввода-вывода в синхронный CPS-преобразованием
** Структуры данных пространства ядра для реализации мультиплексора файловых дескрипторов
** Системные вызовы select, poll, epoll
** Управление скоростью передачи данных через файловые дескрипторы.
* 0 Ticket 12   Synchronisation
** Синхронизация
** Спинлоки
** Ядерные семафоры
** Блокировки и лизинги на файлы
** Структуры данных пространства ядра для реализации блокировок и лизингов
** Системные вызовы: flock, fcntl.
* 2 Ticket 13   Netstack
** 2 Сетевой стек
   Сетевой стек -- весь набор протоколов необходимых для соединения компюьтеров друг к другу.
   OSI -- один из первых базовых сетевых стеков, который не допилили. BSD сокеты -- протокол, который развивался параллельно. Проблема OSI с их разработкой была в планировке, к примеру часть разработчиков хотела разрабатывать пакетную передачу данных (на тот момент новшество), но разработчики из бывшых телефонных компаниях считали эту затею плохой. До пакетной передачи соединение обеспечивалось правильным подключением клиентов друг к другу через свичи.
   Существует три широко используемых протокола низшего уровня: MAC(через Ethernet/DSL), PPP, ARP.
   Первый всем хорошо известен, второй -- это как раз телефоны, третий используется для общения между устройствами в сети для обмена данными необходимыми для других протоколов (IP → MAC).
** 2 PPP
   Point-to-Point Protocol -- протокол, который применяется для соединения двух нодов. Физически используется в телефонии (как стандартной, так и сотовой), оптических сетях и где угодно. Есть два протокола, похожие на PPP: PPPoE, PPPoA -- over Ethernet/over ATM, использующиеся в основном провайдерами чтобы настраивать доступ клиента к внешнему миру.
   PPP пользуется протоколом LCP (Link Control) для того, чтобы установить сессию между узлами (например, пользователем и провайдером).
   PPP поддерживает 3 типа аутентификация для разной защиты. PAP (Password Authentication Protocol) -- протокол для аутентификации пользоватьского пароля на выделенном сервере. Это самый менее секьюрный вариант, пользователь просто отправляет пароль на сервер, а сервер его верифицирует. Ничего не шифруется.
   Есть CHAP (Challenge Handshake Authentication), который отсылает challenge message клиенту на машину, содержащию какую-то рандомную чушь. Машина клиента зашифровывает это вместе с паролем и отправляет обратно на сервер, который это отправляет на сервер аутентификации. Последний зашифровывает challenge с пользовательским паролем с проверяет наличие в базе. Используется модель shared secret, чтобы аутентифицировать пользователя.
   Самый секьюрный вариант -- EAP, про него много.
   PPPoE -- протокол уровня Ethernet, представляющий виртуальное соединение по PPP. Используется провайдерами, как уже было сказано. PPPoE discovery, процесс, который соединяет машины, выглядит так:
   1. Initiation -- клиент высылает специальный пакет на сервер
   2. Offer -- сервер отвечает другим похожим пакетом
   3. Request -- клиент на основании Offer создает пакет и отсылает на сервер
   4. Confirmation -- сервер понимает, что клиент живой, выдает уникальный ID клиенту для PPP сессии и высылает подтверждение клиенту.
** 3 Ethernet
   Самый простой сетевой порт это COM. У него 6 проводов, одна пара из которых передает данные в одну сторону, пара в другую, еще пара сигнальная. Двухпроводное решение широко используется, есть однопроводные, которые очень сложные и непопулярные (чипы в домофонах!). После установления соединения, по проводу передается меандр. Проблемы COM-порта в нарушении целостности сигнала из-за наводки.
   Ethernet реализован чаще всего в формате UTP (unshielded twisted pair) -- много маленьких проводов очень сильно скручены друг между другом. Сигнал по витой паре задается разницой потенциалов (по 2м соседним проводам передаются сигналы, а смысл имеет их разница). Такое решение имеет смысл, потому что любая наводка одинаково искажает сигнал на двух проводах и разность остается неизменной. UTP чаще всего в этой стране встречается в формате двух и четырех пар проводов. Проводов для синхронизации нету, SIGHUB генерируется по таймауту.
   Еще есть коаксиальный кабель (тонкий кабель внутри обертки), профит которого в том, что обертка как-то защищает внутренний кабель от помех, выступая некой клеткой Фарадея.
   Сеть по формату Ethernet до 1Гб/c реализуется подключением всех юзеров к одной общей ethernet-шине. Политика такая: пока кто-то посылает пакеты, другие молчат. Если есть несколько пользователей, которые пользуются шиной одновременно, все замолкают на рандомный интервал времени, потом продолжают. [[https://en.wikipedia.org/wiki/Carrier_sense_multiple_access_with_collision_detection][CSMA/CD]]
   Сеть с большей скоростью нуждается в различных вспомогательных машинах, типа хабов.
   Хаб -- железяка, которая передает пакеты, которые ему приходят, на все свои выходы (броадкастит).
   Есть еще параллельный COM для принтеров, называется LBT. Параллельные шины плохи, потому что сложно синхронизировать передачу данных по многим проводам одновременно, учитывая всякие помехи.

   Ethernet пакет по стандарту (802.3) состоит из:
   1. Преамбула (какие-то метаданные)
   2. Header
      1. Протокол (ethertype) -- есть разные форматы ethernet (LLC, Ethernet II)
      2. Size of packet (or data). MTU -- 1500.
      3. Два MAC-адреса (отправитель и получатель)
   3. Payload (всякие данные)
   4. CRC (контрольные суммы) всего кроме данных
   Больше здесь: [[https://en.wikipedia.org/wiki/Ethernet_frame][Ethernet Frame]].

   Wifi представляет из себя Ethernet по радио. Есть некоторый диапазон частот разрешенный для использования wifi-устройствами, который разделен на поддиапазоны -- каналы. В разных странах используются разные диапазоны. Wifi передает синусоиду, все как по радио, с этими вашими модуляциями.
** 3 IP
   IP протокол находится на уровне выше и обычно запихивается в Ethernet. IP пакет содержит флаги, IP адресата/адресанта, данные и crc для всего кроме данных.
   Сетевые карты обычно фильтруют пакеты которые ей не принадлежат (адрес назначения не совпадает с нашим) на уровне MAC, IP пакеты фильтрует уже ОС. И то и другое поведение может быть изменено с помощью [[https://en.wikipedia.org/wiki/Promiscuous_mode][promiscuous mode]] опции.
   Ethernet bonding — это объединение двух или более физических сетевых интерфейсов в один виртуальный для обеспечения отказоустойчивости и повышения пропускной способности. Гуглится.

   IP -- уникальный идентификатор размером в 4 байта. Подсети бывают классов A, B и C. Для класса A определена маска 255.0.0.0, для B 255.255.0.0, для C 255.255.255.0. Кроме того, определены зарезервированные адреса для сетей: A: 10.0.0.0, B: 172.16.0.0 -- 172.13.0.0, C: 192.168.0.0 -- 192.168.255.0. Маска подсети -- число от 0 до 32, означающее количество единичек перед ноликами в двоичной записи 4-байтового числа. Маска записывается как IP. 24 -- 255.255.255.0, 31 -- 255.255.255.255. Первая нотация называется префиксной (CIDR).
   Работает это следующим образом. Пусть нужно отправить пакет. У каждого интерфейса в компьютере есть своя маска и IP (ifconfig -a). ОС выбирает интерфейс, который наиболее близок по маске с ip с ip адресата (сравниваются and, полагаю). В BSD сокетах это поведение реализуется, если делать bind(0.0.0.0). Можно сделать bind на конкретный интерфейс, и тогда пакеты будут отправляться ровно куда надо.
   Ядро хранит таблицу роутинга, которая говорит, какие пакеты в какой интерфейс пихать (ip r, netstat -rn, route). Есть дефолтный гейтвей (шлюз по умолчанию), в который отправляются пакеты, если они не матчатся по другим маскам (default в route).
** 3 ARP
   Проблема отправки IP пакетов состоит в том, что нам нужны MAC-адреса (можно указать MAC broadcast ff:ff:ff:ff:ff:ff). Для того, чтобы по IP найти MAC, существует протокол ARP. Ядро содержит ARP-таблицу, которая заполняется по мере необходимости и отображает IP в MAC (arp -e). Если в таблице нет записи, а нужно отправить, по сети прогоняется ARP-запрос на уровне "у кого тут такой ip?", и получает ответ.
   Обратный протокол получения IP по MAC первоначально назывался RARP (reversed ARP). Потом он перетек в BOOTP, теперь это DHCP. Существенная разница RARP и DHCP в том, что DHCP -- протокол на уровне TCP/IP, а RARP был на netlink уровне (самом низком). Зачем DHCP оборачивать в IP -- никто не знает.
** 3 Hardware
   Свитч -- это хаб с ARP таблицей внутри, который умеет отправлять пакеты не всем сразу (как хаб), а только тем, кому надо, если в ARP-таблице есть необходимая запись.
   Маршрутизатор -- это свитч с таблицей маршрутизации! Конечно, он тоже имеет ARP, и чаще всего связывает локальную сеть с внешним миром. В таком случае, обычно, в локальной сети у нодов дефолтный гейтвей как раз машрутизатор. Сам маршрутизатор получает свой дефолтный гейтвей обычно от провайдера.
** 3 TCP/UDP/SCIP
   Протоколы, которые обычно запихивают в IP: UDP, TCP, SCIP
   1. UDP, TCP: хедеры, в UDP crc берется от хедеров, в TCP от всего пакета. UDP не обеспечивает никакого механизма проеврки доставки пакета, в отличии от TCP. TCP/UDP пакеты внутри содержат порт (/etc/services).
      Механизм подключения в TCP похож на трехкратное рукопожатие:
      1. Отправляется запрос 1→2 (syn)
      2. Отправляется подтверждение о получении запроса 2→1 (syn-ack, ack = acknowledgment), эта сторона запоминает кому отправила syn-ack
      3. Клиент отправляет 1→2 (ack) еще раз и сервер проверяет, правда ли, что отправлял клиенту syn-ack. Если да, соединение установлено.
      Забавное наблюдение заключается в том, что можно много раз отправлять некоторому набору серверов syn с подмененным ip возврата, и syn-ack будут возвращаться на желаемый адрес, от чего желаемому адресу может стать плохо. Еще минус -- приходится хранить в сервере данные о том, кому отправил syn-ack.
   2. SCTP (stream control transport protocol) -- штука похожая на TCP, но если среди N пакетов некоторые зафейлились, то только зафейленные будут отправляться заново (в TCP все начиная с первого зафейленного). Кроме того, этот протокол подразумевает, что всякие данные для подключения отправляются клиенту от сервера зашифрованными и только сервер может их расшифровать, когда эти же данные ему придут в ack. Отпадает необходимость помнить о syn-ack которые сервер отправляет.

   IPv6 имеет все из коробки внутри. Имеет обратную совместимость с IPv4, зашитый внутрь MAC. Утверждается, что использование IPv6 избавляет от необходимости использовать NAT и DHCP.
** 3 BSD sockets: API, Stream-сокеты, Datagram-сокеты, RAW-сокеты, файловый объект для accept-сокета.
   man socket
   BSD socket API выглядит примерно так (по всему лучше читать man):
   * socket(...) -- создать сокет. Тут устанавливаются всякие параметры, тип сокета (datagram -- UDP, stream -- TCP), другие настройки.
   * connect(...) -- создать соединение на сокете. Первоначально сокет висит в пространстве и ничего не делает, connect его инициализирует.
   * bind(...) -- другой способ инициализации сокета, серверный.
   * listen(...) -- обычно следует за bind.
   * getaddrinfo(...) -- супер обобщенный вызов, возвращающий данные о хосте, которые могут быть использованы для создания сокетов. Прелесть в том, что он удобный и одинаковый для ipv4/v6 сокетов (и еще много чего).
     Есть файл /etc/nsswitch.conf. Сервисы типа getaddrinfo пользуются им чтобы определить откуда искать данные. К примеру, в nsswitch поле hosts хранит "files dns", что соответствует /etc/host.conf и /etc/resolv.conf. Есть демон nscd, который занимается тем, что резолвит запросы "откуда мне бы почитать". Этот демон первоначально запускается от рута и как-то связан с ldap, может резолвить пароли. Есть еще PAM, которой все пользуются (su), и иногда эти сервисы могут конфликтить.

   man socket описывает семейства сокетов как IPv4, IPv6, полезно еще знать про существование AF_UNIX, который используется для общения ядра самого с собой.
   Сокет конкретного семейства имеет тип. RAW сокеты -- это уровень IPv4, но сырой, без части хедеров. Поскольку с такими сокетами можно набагать и застопорить какую-нибудь очередь IO, они доступны только руту.

** 3 ICMP, TFTP, DNS, NAT
   ICMP протокол, который завернут в Ethernet, используется для общения между роутерами, логирования ошибок, для ping/traceroute. IP пакеты имеют TTL и на каждом hop отправляют запрос обратно.
   TFTP -- UDP-протокол, обеспечивающий наивную реализацию того, что делает FTP (достань-ка мне тот файл).
   DNS -- /etc/resolv.conf. Та самая штука, которая мапит имена в <host,port>. Самый простой вариант использовать DNS -- gethostbyname -- как раз получает IP по хосту. DNS пакеты имеют тип и имя. Типы: A(IPv4), AAAA(IPv4), MX(email), TXT(что угодно). Именем является хост. Ответы бывают рекурсивными и нет. Рекурсивные ответы возвращают кучу ip-адресов, соответсвтующих одному хосту (например, сервер распределяется между несколькими хостами для уменьшения нагрузки).
   NAT (network address translation): пусть есть локальная сеть и мы пользуемся внутри локальной адресацией. Тогда если узел отправляет пакет во внешний мир, он проходит через шлюз по умолчанию. Устройство, которое имеет адрес шлюза, содержит таблицу, которая сохраняет данные о пакетах. Устройство подменяет source пакета на свой, и отправляет куда надо. Когда возвращается ответ, он перенаправляется юзеру согласно таблице. Существуют хаки, которые позволяют отправлять пакеты напрямую. Гуглить tsocks, UPnP.
* 0 Ticket 14
** Терминалы и управление заданиями в POSIX
** Терминалы, псевдотерминалы и режимы их работы
** Группы процессов и сессии
** Foreground и background группы
** Сигналы: INT, HUP, TSTP, TTIN, TTOU, WINCH
** Демоны и демонизация.
* 2 Ticket 15   Booting
** 2 Pre-BIOS: хардварная загрузка
   Материнская плата имеет огромное число всяких разных защелок, кнопка включения/reset приводит их в детерменированное состояние. Затем подается питание на процессор и указатель направлен в константную память, в которой лежит BIOS.
   Тут Ян минут 5-10 рассказывал, но вроде не критично и не нужно.
** 3 Загрузка: BIOS → MBR (DOS Label), DOS/Windows boot, GRUB
   Первым делом BIOS инициализируется. Затем иницализируется VGA BIOS -- штука которая инициализирует VGA-контроллер. Происходит проверка системы на целостность, прогоняются тесты. Существуют различные вариации селф-тестов, в зависимости от желаемого времени прохождения. Первоначально целью этих тестов было получить размер оперативной памяти (программа подсчитывала количество байтов линейно).
   Далее проверяется наличие всех необходимых контроллеров. Ровно тут существовала популярная ошибка "no keyboard detected" -- старые операционные системы не могли работать без клавиатуры. Более того, раньше никто не задумывался о необходимости запускать ПК без клавиатуры или видеокарты, так как самый популярные юзкейс -- серверы, а раньше из обычных ПК серверы никто не делал, там железо требовалось особое. Кстати говоря, клавиатуры подключались через PS/2 -- он очень простой, в этом профит.
   Если у BIOS происходят какие-то ошибки, понять, какие конкретно, сейчас можно по специфическим гудкам, которые он издает с помощью встроенного динамика (и документации). BIOS можно дампить, он там свой стейт как-то в CMOS хранит.
   После прохождения self-тестов BIOS предоставляет возможность что-то сделать, войти в какой-нибудь GUI по нажатию f11, например. Раньше кастомизация BIOS происходила с помощью джамперов, которые выставлялись один раз перед загрузкой.
   Затем происходит загрузка с дефолтного boot устройства. Тут необходимо посвятить время основной загрузке с жесткого диска и немного сетевой загрузке.

   При сетевой загрузке используется PXE. У нас есть сетевая карта, драйвер к которой давным-давно был расположен на самой кате, а сейчас он есть в BIOS. BIOS может с ее помощью вытаскивать необходимые данные. Есть несколько вариантов PXE на данный момент, самый популярный -- pxelinux, или ipxe. Первый поддерживает TCP, и это очень круто, потому что TFTP по UDP может терять пакеты и если ядро большое, можно много раз безуспешно пробовать его загружать. Частая практика с PXE использовать chained requests. Один PXE вытаскивает с сервера некоторый код, который предоставляет GUI для того, чтобы выбрать другой удаленный сервер и выбрать ядро, которое тебе нравится (например).

   Стандартная загрузка с жесткого диска происходит следующим образом:
   BIOS загружает необходимые ему драйвера: ATA-IDE, SATA-SCSI, USB. USB драйвера труднее писать из-за того, что USB необходимо постоянно поллить (значит ли это, что драйвера для USB не всегда включены в BIOS?). Затем BIOS грузит в оперативную память первые 512б и загружается с них. Есть несколько вариантов разметки жесткого диска, которые позволяют делать разные приятные вещи, к примеру MBR или GPT. Отметим, что нет ничего противозаконного загружаться напрямую из какого-то кода (как grub или ваш_кастомный_загрузчик).
   MBR (DOS label) формат: 512b. Последние два байта это 0x55AA, необходимы для первоначальной проверки того, что шина работает. Кроме того, это индицирует, что диск размечен MBR а не чем-то другим. Первые 510б -- это jump, метаданные, загрузчик и TBL. Jump просто перепрыгивает метаданные. Все, что делает код -- загружает нужный кусок памяти и запускает его.
   TBL (загрузочная таблица) содержит 4 основных (primary) раздела, для каждого определено boot bit, тип, старт (адрес) и длина. Boot bit показывает, что с этого раздела нужно грузиться. Кроме 4 основных разделов можно добавить еще некоторые дополнительные (extended).
   Любой раздел кроме уже перечисленного содержит (в таблице) свой тип и другие данные. Linux игнорирует TBL-информацию о типе, но для DOS это критично. Обычно ядро лежит по фиксированному адресу на диске. Важное замечание: fdisk не дает создать раздел раньше чем некоторый оффсет с начала диска по причине того, что начало обычно резервируется для MBR + еще потенциально чего-нибудь. Кроме того, даже начало MBR не совпадает с началом диска, а есть еще оффсет, который свойственен для конкретной модели HDD ввиду того, что дорожки близко к центру плохо отцентрованы.
   GRUB обычно устанавливается как раз сразу за MBR и занимает секцию кода в MBR. Также GRUB содержит рядом со своим исполняемым кодом различные драйвера. GRUB похож на маленькую OS, которая загружает разделы с помощью драйверов которые вот там есть и показывает GUI, предоставляя возможность настраивать все, что настраивается.
   Забавный факт: с некоторых пор GRUB начал хранить себя еще и с crc, по причине того, что Windows никак не защищает этот кусок памяти, и туда могут благополучно писать кто хочет, в том числе и Photoshop, который хранит где-то в этом месте свои ключи регистрации, чтобы пользователи после переустановки системы не могли сбросить лицензию.

   С GPT памяти на то же, что использовалось в MBR, намного больше. В начале есть MBR-совместимая таблица, потом располагаются 512 ячеек TBL.
** 3 initrd
   Initrd -- это cpio архив, который грузится в память, а ядро затем монтирует это как дефолтную систему. Основная цель initrd -- обеспечить дополнительную функциональность, когда слишком сложно/лень писать новый модуль ядра. Initramfs -- это модификация initrd, доступная в linux с 2.6.13, которая монтируется как tmpfs.
   Пусть у нас есть ядро и initrd. Ядро обычно находится по некоторому фиксированному адресу. Ядру передаются параметры, в том числе root={dev|UUID}, который говорит, что монтировать в корень (blkid). Ядро обычно сжато bzImage, с тех пор, как оно стало достаточно большим -- это довольно специфичный архивный формат (не связан с bzip2), основанный на gzip. В начале этого архива есть программа для разархивации.
   GRUB запускает ядро, распаковывает initrd и монтирует его в корень (/). Затем запускается /init, который дает старт загрузке. Всякие встроенные устройства типа роутеров как раз имеют ровно ядро и initrd, которые лежат в некоторой NVRAM. В таких встроенных системах пользуется популярностью busybox -- программа, которая имитирует стандартный набор утилит linux (парсит 0 аргумент и запускает что надо). В случае, если установлен busybox, /bin/{ls, mv, cp, cat} -- symlink'и на busybox (busybox --help, busybox --install -s dir -- устанавливает симлинки на себя). Цель busybox -- иметь кучу всего, при этом не тратя много памяти (стандартные утилиты имеют достаточно ограниченный функционал, меньший, чем оригиналы).
   После этого создаются /dev/{stdin, stdout, stderr, console} и console выставляется на все стандартные std... (exec < /dev/console && exec > /dev/console && exec 2> /dev/console). Следующим шагом возникает необходимость примонтировать какую-нибудь файловую систему, и это делают двумя способами:
   1. mount -t procfd ... /proc, mount -t sysfs ... /sys; launch udev.
   2. Монтируется некоторая специфическая система (Ян не вспомнил названия), которая имеет udev внутри и создает все inod'ы автоматически.

   udev(7) -- это демон, который создает netlink (socket(2)) сокет с ядром, в которое ядро дампит информацию про устройства, а потом парсит эти данные, классифицирует (connect/disconnect/modify) и согласно правилам в /etc/udev что-то делает (чаще всего создает что-то в /dev, переименовывает или меняет симлинки). Существует также поведение udev, которое называется settle (udevadm(8)) -- udev обрабатывает всю очередь событий и выходит.

   После этого можно отмонтировать себя и загрузить желаемый раздел (а между тем что-нибудь еще расшифровать или сделать еще что-нибудь интересное, что позволяет initrd). Если используется busybox, то определить файловую систему помогает blkid, если нет, то полноценный mount сам может. Есть еще проблема с инициализацией SATA, так что blkid умеет ждать в цикле инициализации. Альтернативный подход к решению проблемы -- libsata модуль, которым никто не пользуется, потому что никому не нужен модуль, который ждет SATA и блокирует систему.
   Теперь мы можем примонтировать / и запустить init. Сделать это можно с помощью pivot_root && exec /sbin/init. Внутренний init делает что-то специфическое, свойственное для системы. В этом месте как раз мы расшифровываем диск, если он зашифрован. Можно тут загрузить вместо init просто emacs, который умеет делать сам практически все необходимое, тогда в /bin кроме него нужно положить еще mount, а busybox'а с головой должно хватить (тут много шутят про emacs OS still needs a better editor -- возьмитв evil/viper/vimpulse с собой!).

   Рассмотрим пример с USB:
   1. USB воткнут в порт.
   2. Проходит 300мс, необходимые для того, чтобы убедиться, что USB всунут плотно (лол).
   3. Контроллер на флешке понимает, что он подключился куда-то и отправляет сигнал
   4. Проходит через южный мост
   5. Через северный
   6. В процессор, который получает прерывание
   7. Ядро обрабатывает прерывание, смотрит на контроллер, понимает какой драйвер нужен
   8. Смотрит в табличку специальную, осознает какой модуль за это отвечает (если в ядре нету драйвера)
   9. Hotplugging: в ядро загружается код, который представляет из себя нужный модуль (код мерджится с кодом ядра), с зависимостями. Или все падает, если чего-то нет, хотя обычно утилиты конфигурации ядра (menuconfig в gentoo) такого не допускают, компилируя все зависимости.
   10. создается sys/..., в дело вступает udev и создает /dev/sdd{..}
   11. Все последующие прерывания обрабатываются уже из загруженного модуля.

   Все операции во время hotplugging'а происходят с помощью UNIX сокетов, которые должны быть вкомпилированы в ядро, иначе мы получим бесконечный цикл попыток загрузить модуль с UNIX сокетами.
   Все модули загружаются автоматически, но иногда приходится делать это вручную. Например, с помощью /etc/init.d/modules.
   Firmware загружается напрямую в железо (как например драйвер видеокарточки), и затем появляется возможность общаться с картой через стандартный интерфейс (opengl какой-нибудь).
** 2 Инициализация системы: последовательная, учитывая зависимости, resource/socket activation, lazy activation, cтандартные init системы
   Последовательная инициализация -- запустить все (!!!) последовательно! С зависимостями -- имеется некоторый набор сервисов, которые превращаются в граф. Resource activation -- не запускать сервер, пока не будет в том необходимости, то есть клиенты раньше сервера. Lazy activation -- полагаю, что приоритеты, как в systemd.
   Init системы:
   Первоначально был System V init -- демон, который создавал /dev/initctl сокет при старте. Можно отправлять в этот сокет команды запускать runlevel'ы (определены в /etc/inittab). Типа ты запускаешь runlevel с помощью rc N команды, и тогда rc запускает /etc/rc<N>.d/*K kill, потом /etc/rcN.d/*S start, смена runlevel'а останавливает все предыдущие процессы. Дефолтно 0 -- halt, 1 -- single user, 2 -- многопользовательский без сети, 3 -- многопользовательский с сетью, 6 -- reboot.
   System V init вполне себе ОК, когда задач не очень много, так что его используют на всяких встроенных системах, читалках и тд. Вторая проблема -- это демоны, их трудно трекать.
   Перед systemd, arch linux пользовался какой-то модификацией systemv с поддержкой асинхронного запуска программ.
   Первое нормальное решение вместо system v -- это Upstart, штука очень похожая на system v, но умеющая трекать демонов и мультизадачная, на основе событий -- некоторые скрипты создают евенты, которые другие события слушают, так что можно запускать что-то асинхронно. На события можно подписываться.
   Socket activation -- решение создавать все сокеты и каналы перед выполнением задач, а потом все сразу запустить. Такая штука использовалась некогда в Mac OS.
   Systemd -- init с поддержкой мультизадачности на графе сервисов, ребра которых либо сокеты, либо просто непосредственный запуск ресурса. Это более оптимально чем socket activation из-за того, что вершины имеют приоритет + проблемы с демонами решены с помощью механизма cgroups. Cgroups представляет собой набор процессов, объединенных круче, чем обычные группы процессов, а именно: можно ограничить группе доступ к памяти, дать группам разный приоритет по отношении к CPU/IO, можно убивать, чекпоинтить и рестартить всю группу сразу. Проблема демонов решена ровно потому, что из cgroup нельзя просто так выйти.
   Openrc -- gentoo init, который очень похож на systemd, но без безумных идей.
   # Тут нужно сказать почему systemd и kdbus -- очень ему не нравятся
** 2 Стандартные демоны: init, syslog, klog, cron, at, ssh
   1. init daemon -- уже обсудили
   2. syslog - демон, который читает из ядра логи и пишет их
      Поговаривают, он читает /dev/log сокет и делает что-то согласно /etc/syslog.conf
      http://www.k-max.name/linux/syslogd-and-logrotate/
      На gentoo все пользутся syslog-ng или rsyslog, которые умеют делать что-то с логами согласно конфигурации -- класть их в /var/log или пересылать по сети.
      https://wiki.gentoo.org/wiki/Rsyslog
   3. klog
      Тоже логгер какой-то видимо, про него ничего нету в интернетах
   4. cron
      Супер-полезная штука, которая делает какие-то вещи по расписанию, будь то бекапы или обновления, или еще что угодно
   5. at
      Демон atd висит и исполняет команды, которые его попросили (единажды).
   6. ssh
      sshd(8) и ssh -- программы, которые позволяют установить зашифрованное сообщение на незашифрованной сети
** 2 Стандартные файлы /etc: fstab, mtab, sysctl.conf, motd, issue, nologin.
   1. fstab -- file system table, файл который говорит, какие разделы куда нужно монтировать при init'е
   2. mtab -- mounted table, там написано что сейчас замонтировано и как
   3. sysctl.conf -- содержит настройки, которые необходимы sysctl для смены конфигурации ядра в рантайме
   4. motd -- (message of the day) все что там написано выводится после успешного login
   5. issue -- выводится до логина
   6. nologin -- если /etc/login существует, то логиниться можно только root'у
* 0 Ticket 16
** Запуск программ, динамическая линковка и загрузка
** Exec magic и интерпретаторы
** Релокация кода: релокационные дырки, кеширование релокаций, GOT, PIC
** Объектные, исполняемые и библиотечные файлы
** Формат ELF
** ld-linux и его x86 32 шные ужасы.
* Uncategoriesed
** Execution levels
   There's -2 level of execution, for example something that governs how to operate with cooler.
   Available protocols for hard drives and stuff:
** Encryption and safety
   TPM -- hardware, that has 256-bit registers, near 20 items of them, has clear operation and extend reg data operation -- takes hash from data, hashes it with register and writes to it. TMP has some processor, that can clear, extend, and built-in algorithms of encryption, like AES (very secure, there's a proof, thats energy to decrypt it is more than the energy to melt the Earth: http://www.eetimes.com/document.asp?doc_id=1279619). There's also seal operation, that takes registers, data, and uses registers to encrypt data, then puts data into one of hardware box; There's also unseal operation, that does the opposite.
   Hardware encryption: LUKS, 2mb of data in the start of disk, that contains metadata, salt, master-key, header, algorithm for hashing passwords and disk, and then it's some magic. The disk is always encrypted. Hash cache is algorithm of detecting SPAM -- user that sends email generates hash collisions and it takes lot of computer time.
   That's not clearly secure (LUKS) because one can replace initrd. That's called evil maid strategy. We can encrypt all except grub, but there can be an malware in grub. There can be a solution with outer flash drive, that contains everything but encrypted binary blob (even LUKS header). With TPM it's easier, because things that BIOS does are extended. BIOS CAN HAVE MALWARE TOO!! PARANOID!! TXT is a technology by Intel that allows you to trust your processor, and it uses TMP to extend some hardware id.

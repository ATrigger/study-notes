#+TODO: X 0 1 2 | 3
#+TITLE: Tickets for exam on operating systems lectures course given by Yan Malakhovski, 4th term, 2015

* Legend
  X -- нифига не написано и непонятно, где брать
  0 -- нифига не написано
  1 -- что-то написано, но не дописано
  2 -- написано, но возможно, что что-то серьезно упущено
  3 -- все хорошо.
* Sources
  Тут только самые важные и обширные источники. Всякие статьи, которые по мере необходимости встречаются в тексте, встречаются только там (и в заголовках тем).
  1. Raw notes by volhovm
  2. OS course plan
     http://rain.ifmo.ru/~trojan/linux/year2009/
  3. Yet another plan
     http://rain.ifmo.ru/~trojan/linux/year2007/
  4. Prev. year googledoc
     https://docs.google.com/spreadsheets/d/1CoPjN7shou3m3kAQdsRHY1HxQYyKho5f7Qn1KeL9n-U/edit#gid=0
  5. Malakhovski's notes
     https://github.com/oxij/unix-notes-ru/blob/master/compiled/main.pdf
  6. Wikipedia ofc
  7. Very much about memory
     [[http://www.akkadia.org/drepper/cpumemory.pdf]]
* 2 Ticket 1    RAM
** RAM, SRAM, DRAM
   RAM (random-access memory) - это оперативная память.
   От постоянной памяти ее отличает:
   1) скорость работы
   2) потеря данных в отсутствие питания
   3) да вообще все по-другому!

   SRAM (static random-access memory) - статическая память.
   Ячейка SRAM основана на защелке: [[http://2.bp.blogspot.com/-dCCrTGB-c6U/T1zaY5TG1oI/AAAAAAAAAu8/MutoYbjglvs/s1600/SRAM.gif][схема]]
   *Read-операция*: мы даем на WL напряжение, сигнал уходит с BL и ¬BL.
   *Write-операция*: мы даем на WL напряжение и в BL тоже даем сигнал, он запоминается.

   Pros:
   + Быстрая
   + Не надо ничего перезаписывать
   Cons:
   - Защелки жрут электричество все время
   - Схема ячейки сложная, делать дороже
   - Ячейка занимает много площади

   DRAM (dynamic random-access memory) - динамическая память.
   Ячейка DRAM основана на конденсаторе: [[https://www.cs.auckland.ac.nz/~jmor159/363/html/fig/dram_cell.gif][схема]], [[https://upload.wikimedia.org/wikipedia/commons/3/3d/Square_array_of_mosfet_cells_read.png][схема банка]]
   *Read-операция*: мы даем на WL напряжение, конденсаторы разряжаются, сигнал уходит,
   и теперь его надо перезаписать обратно.
   *Write-операция*: даем на WL и BLs напряжения, те конденсаторы, на которых 1, заряжаются,
   а те, на которых 0 - разряжаются.
   *Замечание*: так как конденсаторы разряжаются сами по себе, периодически надо их перезаписывать
   (memory refresh)

   Pros:
   + Дешевая
   + Ячейки маленькие, можно много понапихать
   + Жрет энергию только во время read/write и refresh
   Cons:
   - Медленная (конденсаторы разряжаются не мгновенно)
   - Refresh-и тоже не прибавляют отзывчивости

** Структурно-операционная схема обычной планки памяти (DDR)
   Схема одного банка: [[https://upload.wikimedia.org/wikipedia/commons/3/3d/Square_array_of_mosfet_cells_read.png][схема банка]]
   Банки лежат на планочке рядом. Верхняя часть адреса ячейки отвечает за номер банка

   DDR (double-data rate) - хитрость, позволяющая передавать 2 слова за такт процессора вместо 1.
   Хитрость в том, что мы передаем данные как на восходящем, так и на нисходящем фронте меандра.
** Кеши CPU: L1, L2, L3
   Кэш - быстрый кусок SRAM рядом с процессором, в который складываются часто используемые данные. Кэш - всегда SRAM, чтобы было быстро, и потому всегда маленький, чтобы было не так дорого и энергозатратно.

   L1 - это самый маленький и близкий к процессору кэш. Он обычно сидит на том же куске кремния. Иногда подразделяется на L1i (кэш инструкций) b L1d (кэш данных). Его подстраховывает кэш L2 - который побольше и чуть подальше.
   Для многоядерных процессоров есть также кэш L3, к которому могут обращаться все ядра. (подробнее про это все - в последнем пункте)
   Кэши могут быть inclusive (L1 ⊂ L2 ⊂ L3 - данные дублируются) или exclusive (данные не дублируются).

   Кэш разбит на кэш-линии. Структура кэш-линии:
   | tag | data block | flags |
   Плюс известен номер кэш-линии.
   Мы делим:
   * tag - кусок адреса того куска памяти, который дублируется в этой кэш-линии. За тегом закреплен некоторый последовательный регион данных размера 2^(length tag). Нужен, чтобы искать данные по адресу в кэше.
   * data - собственно, сами данные (может, 256 байт, много).
   * flags - различная мета-инфа, а еще тут бывают коды коррекции ошибок.

   Обычный адрес в памяти выглядит так:
   | tag | index | offset |
   * tag - как раз тот кусок, который матчится с tag в кэше.
   * index - индекс кэш-линии, определяет, в каком наборе линий искать данные.
   * offset - отступ от начала линии.

   Контроллер обычной памяти же воспринимает адрес по-своему (как?)

   По методике синхронизации с памятью кэши бывают write-through и write-back. write-through - это когда любой запрос на запись всегда досылается в память, write-back - это когда данные из кэша дампятся в память только при вытеснении.

   sources:
   [[https://en.wikipedia.org/wiki/CPU_cache]]
   [[http://arstechnica.com/gadgets/2002/07/caching/2/]]
** Схема параллельного извлечения
   Параллельное извлечение используется в кэше (там где у кэш-линий есть tag). Мы просто посылаем tag адреса на компараторы кэш-линий. Компаратор сравнивает tag линии с переданным, и высылает 1, если он совпал, и 0 - если нет.

   Данные извлекаются из всех кэш-линий одновременно, и данные каждой кэш-линии and-ятся с результатом компаратора. Выходные провода данных спаиваются вместе, и в итоге на выходе получаются только данные из кэш-линии с нужным тэгом.

   Если же все компараторы вернули 0, то мы детектируем cache miss и перенаправляем запрос к контроллеру памяти.

   sources:
   [[http://www.csbio.unc.edu/mcmillan/Media/L20Spring2012.pdf]]
   [[http://lwn.net/Articles/252125/]]
** Извлечение демультиплексором
   Демультиплексор - это штука, которая принимает n-битное число и сигнал и дает этот сигнал на соответствующий числу выход (один из 2ⁿ).
   [[https://en.wikipedia.org/wiki/Multiplexer#/media/File:Demultiplexer_Example01.svg][Схема демультиплексора]]
   Извлекать данные демультиплексором надо так: [[http://lwn.net/images/cpumemory/cpumemory.9.png][схема извлечения]].

   Row Address Selection - демультиплексор (выбираем строку но номеру).
   Column Address Selection - мультиплексор (выбираем столбец по номеру из тех, что пришли).

   Демультиплексор также используется в кэшах с неполной ассоциативностью, где с его помощью извлекаются кэш-линии с заданным индексом.
** Ассоциативность
   Суть кэша - в том, чтобы быстро проверить, лежат ли данные по заданному адресу в кэше, и если да - вернуть их.
   Если кэш полностью ассоциативный (fully-associative), то мы должны сделать параллельное извлечение сразу из всего кэша! С этим есть несколько бед:
   - Теги должны быть большими, слишком много места в кэше отведено под тэги
   - Теги большие - и компараторы для них тоже большие, а значит - дорогие и медленные

   Однако, есть плюсы:
   + Не бывает коллизий кэша - если мы хотим положить что-то в кэш и в нем еще вообще есть место, нам это удастся
   + Как следствие этого, происходит мало cache miss-ов

   Противоположность: direct-mapped cache.
   Адрес в памяти однозначно определяет кэш-линию, в которой эта ячейка памяти может содержаться.

   Делается это так: адрес демультиплексируется по index, выбирается 1 кэш-линия,
   тэг в ней сравнивается с данным, и если тэг равен - данные кэш-линии возвращаются.
   Иначе - регистрируется кэш-мисс.

   Pros:
   + Все это делается быстро, нет кучи компараторов
   Cons:
   - Так как каждая ячейка памяти может быть сохранена только в 1 кэш-линии,
     возникает множество *коллизий* - это когда 2 разные ячейки попадают в одну
     и ту же кэш-линию и кто-то из них вытесняет другую
   - Вследствие этого, много кэш-миссов

   Золотая середина - n-ассоциативный кэш, когда каждая ячейка может содержаться в
   одной из n линий. Делается это так:
   1) Демультиплексор отсеивает n кэш-линий по index адреса
   2) Из этих n линий по тэгу извлекаются параллельно данные

   sources:
   [[http://www.csbio.unc.edu/mcmillan/Media/L20Spring2012.pdf]]
   [[http://arstechnica.com/gadgets/2002/07/caching/5/]]
   [[http://arstechnica.com/gadgets/2002/07/caching/6/]]
** TLB
   TLB - translation lookaside buffer - это такой специальный кэш, который маппит виртуальные адреса в реальные.
   Он небольшой, и, конечно, не содержит все используемые виртуальные адреса, а лишь часто используемые.
   Трансляцией из виртуальных адресов в реальные занимается MMU - memory management unit - специальный кусок процессора.
   Он глядит в TLB, и если не находит там, пускается в долгий путь по page table-ам в основной памяти
** Общее влияние кеша на работу с памятью
   Кэш, в целом, ускоряет работу с памятью (кто бы мог подумать?). Иногда получается так, что работа идет только с кэшом, а к памяти обращений и вовсе нет (в случае look-aside).
   ?? Что здесь написать ??
** Кеши в мультипроцессорных системах и когерентность кешей
   Если у нас есть много ядер, то у каждого ядра есть собственные кэши L1 и L2.
   Однако, что же делать, если одна и та же ячейка памяти продублирована в кэшах разных ядер, и одно ядро меняет эту ячейку в своем кэше?
   Другое должно как-то увидеть это изменение.

   Для таких ситуаций существует *протоколы когерентности кэша*. Например, MESI:

   Во flags каждой кэш-линии кодируется ее состояние, 1 из 4:
   * Modified  - актуальная кэш-линия есть только в этом кэше, и она была изменена, то есть не соответствует данным в основной памяти.
   * Exclusive - кэш-линия актуальна только в этом кэше, и она совпадает с данными в памяти.
   * Shared    - кэш-линия совпадает с данными в памяти и может присутствовать в нескольких кэшах.
   * Invalid   - кэш-линия невалидна.

   Read может происходить из любого состояния, кроме Invalid. Если пытаемся читать из Invalid, то нужно сначала пофетчить данные (извлечь из соседних кэшей или из памяти). После фетча Invalid сменится на Shared.
   Write может происходить только в Modified или Exclusive. Иначе сначала нам нужно инвалидировать все копии в других кэшах, а потом выставить статус Modified.
   Перед инвалидацией Modified-линии нужно сбросить данные из нее в память (write-back).

   sources:
   [[https://en.wikipedia.org/wiki/MESI_protocol]]
* 2 Ticket 2    CPU pipeline
** Пайплайн и стадии
   Исполнение инструкций - сложный процесс, включающий в себя много этапов.
   Чтобы было быстро, процессор разбивает инструкции на этапы и выполняет их на конвейере (pipeline)

   Этапы конвейера:
   1) Fetch
      Получение инструкции по адресу, на который указывает IP. Обычно достается
      из кэша L1i
   2) Decode
      Декодирует полученную инструкцию и, таким образом, определяет, что делать дальше
      (сколько аргументов фетчить, куда их посылать и так далее
   3) FetchArgs
      Получает все аргументы инструкции (в том числе, вычисляет effective address)
   4) Execute
      Непосредственно выполняет инструкцию
   5) Commit
      Записывает результаты в регистры/память

   [[file+emacs:pipeline.hs][Модель пайплайна на Хаскеле]]

   sources:
   [[https://en.wikipedia.org/wiki/Classic_RISC_pipeline]]
** Регистровый файл
   Регистровый файл - SRAM-массивчик в процессоре. Содержит в себе ячейки с регистрами.
   В простых процессорах имена регистров в коде напрямую маппятся в эти ячейки, в процах покруче (современных) они иногда просто переименовываются (так себя ведет, например, процедура XCHG (кажется)).
** Пузыри (pipeline bubbles)
   Пузырь - это последовательность nop-ов. Он появляется, когда только что зафетченную инструкцию нельзя сразу начать исполнять - например, она зависит от результата предыдущей, которая еще не завершила выполнение.
   Появление пузырей - это самый простой способ решения data hazard (проблемы зависимостей данных) и control hazard (проблемы переходов)
   Другим способом решения data hazards является forwarding - отсылка полученного результата инструкции назад. Между каждой стадией есть буфер в который кладется промежуточный результат и процессор на стадии fetchArgs может обращаться к этим буферам наперед. Есть еще документирование, которое работает как "А давайте вы не будете так делать", например если VLIW.

   sources:
   [[https://en.wikipedia.org/wiki/Bubble_(computing)]]
** Предсказание переходов (branch prediction)
   Когда процессор натыкается на инструкцию j* (условный переход), он должен ее распарсить и выполнить, прежде чем станет ясно, куда нужно сдвинуть IP. Однако, если простаивать все это время, пока инструкция не выполнится, будет очень долго. Поэтому процессор пытается угадать, куда все-таки в итоге нужно будет прыгнуть, и начинает фетчить и выполнять инструкции оттуда. Если же предсказание было неверно, весь пайплайн сбрасывается, и инструкции фетчатся заново с верного адреса.

   Отсутствие branch-prediction'a - это когда IP всегда просто сдвигается на единичку вперед. Чуть более продвинутые процы, кстати, обрабатывают безусловные переходы (jmp) на decode-стадии, иначе бы каждый безусловный переход был долгим и мучительным.

   Самый простой нетривиальный бранч-предиктор - это 2 бита на каждый джамп, которые олицетворяют одно из 4 состояний:
   | True | Almost true | Almost false | False |
   Если в результате выполнения условия мы все-таки прыгнули, тогда состояние предиктора сдвигается влево. Иначе - вправо.
   Понятно, что если предиктор находится в состоянии True/Almost true, то следующие инструкции мы фетчим из адреса прыжка.
   Иначе - следующие инструкции фетчатся со следующего адреса.

   В современных компуктерах branch predictor'ы гораздо более хитрые, там таблицы всякие, но достаточно этого сказать, пожалуй.
** Out of order исполнение
   Out-of-order execution - это технология, позволяющая динамически изменять порядок выполнения инструкций.
   Суть проста: предположим, есть следующий порядок инструкций:

   ADD rax, rbx
   SUB rcx, rax
   ADD r8, r9

   Вторая инструкция зависит от 1, поэтому должна ждать ее выполнения. Однако 3 инструкция не зависит ни от той, ни от другой - почему бы не пропустить ее между 1 и 2?

   Процессоры с out-of-order исполнением имеют буфер инструкций (reservation station) и очередь результатов. Обработка инструкции происходит так:

   1) Инструкция фетчится и декодится
   2) Инструкция кладется в буфер
   3) Когда что-то в регистрах/памяти меняется, в буфере ищутся инструкции, которые от этого "чего-то" зависели. Достаем самую старую из них.
   4) Инструкция выполняется
   5) Ее результат кладется в очередь
   6) Результаты записываются в регистры/память в порядке очереди

   sources:
   [[https://en.wikipedia.org/wiki/Out-of-order_execution]]
   [[https://en.wikipedia.org/wiki/Reservation_station]]
** Интерфейс между устройствами ввода и CPU
   [[https://upload.wikimedia.org/wikipedia/commons/thumb/b/bd/Motherboard_diagram.svg/665px-Motherboard_diagram.svg.png][Схема общения CPU со всем остальным]]

   Процессор общается со всеми остальными устройствами через чипсет материнской платы. Чипсет - это 2 устройства - Northbridge (NB) и Southbridge (SB)
   NB соединяет CPU с критичной по скорости периферией:
   * памятью
   * видеокартой на AGP/PCI-Express
   * а также с SB

   SB соединяется со всей остальной периферией:
   * PCI-девайсами (сетевая/звуковая карта)
   * PCI/2 периферия (старые клавы и мышки)
   * жесткими дисками (IDE, SATA)
   * ROM (память BIOS)
   * часы

   Также иногда SB соединяется с встроенной сетевой/звуковой/видеокартой.
** Прерывания
   Прерывания - это механизм сообщить процессору о том, что что-то произошло и на это надо отреагировать.

   Прерывания бывают следующие:
   1) Hardware
      1. Игнорируемые (IRQ - interrupt request) - прерывания, имеющие маску, по которой их можно опознавать и игнорить (почти все хардварные прерывания такие)
      2. Неигнорируемые (NMI - non-maskable interrupt) - прерывания без маски, которые нельзя игнорить (e. g. watchdog timer - строгий контроллер таймаута)
      3. Межпроцессорные - генерируемые одним процессором/ядром для другого
   2) Software - генерируемые самим процессором из инструкции int, как правило, используются для написания сисколлов.

   В железе прерывания реализованы как level-triggered и edge-triggered.
   level-triggered: устройство выставляет напряжение на проводе запроса прерывания и держит его, пока ему не откликнутся
   edge-triggered: устройство посылает импульс на провод запроса, и южный мост запоминает, что это произошло.

   Есть еще контроллер прерывания, который специальная железка на плате, имеющая приоритеты и умеющая отсылать только нужные прерывания, чтобы все не приходило в процессор.
   sources:
   [[https://en.wikipedia.org/wiki/Interrupt]]
** DMA
   DMA - direct memory access - это технология, позволяющая устройствам общаться с памятью в обход процессора.
   Когда видеокарта, например, хочет достать какие-то данные из памяти, она посылает interrupt процессору, а он уже достает все из памяти и отсылает видеокарте. Но на это тратится его драгоценное время!
   С DMA можно так: процессор передает работу DMA-контроллеру и занимается своими делами. Тот же перекладывает данные из одного места в другое и посылает процессору interrupt об окончании.
   DMA можно использовать также и для копирования данных из одного куска памяти в другой!
* 2 Ticket 3    Virtual memory
** 3 Виртуальная память
   Виртуальная память - механизм, который отвечает за следующие задачи:
   1) Изоляция памяти процессов друг от друга и от памяти операционной системы
   2) Представление памяти для каждого процесса как единого непрерывного куска
   3) Использование большего количества памяти, чем физически возможно

   Суть в том, что каждому адресу в программном коде (виртуальному) сопоставляется физический адрес в плашке памяти. Причем таблица сопоставления виртуальных адресов для каждого процесса своя.
   Кроме того, виртуальная память может мапаться куда-нибудь еще, например - на диск, что позволяет создавать swap partitions.

** 3 MMU: TLB, каталог страниц (page table)
   MMU - memory management unit - устройство, управляющее трансляцией виртуальных адресов в физические.
   При нем есть его собственный кэш - TLB (translation lookaside buffer) - который хранит небольшое множество часто используемых адресов.

   Если в TLB записи нет, идет поиск по таблицам страниц (page tables -- paging). Обычно они находятся в основной памяти, но в некоторых MMU есть для них рядышком отдельный буфер.
   Обычно page tables устроены так ([[http://users.dickinson.edu/~braught/courses/cs354f97/Classes/Class17/Image63.gif][схема]]):
   * первые 10 бит адреса индексируют запись в таблице 1-го уровня (таблице таблиц). В записи лежит индекс страницы 2-го уровня.
   * вторые 10 бит адреса индексируют запись в таблице 2-го уровня. В записи лежит адрес начала 4-килобайтной страницы
   * последние 12 бит - номер байта в странице

   Если в таблице не нашлось нужной страницы, MMU делает либо запрос в swap (если он настроен), либо говорит процессору, что случился page fault.

   sources:
   [[https://mn.wikipedia.org/wiki/Page_table]]
   [[https://en.wikipedia.org/wiki/Page_table]]
** 1 Биты: readable, writeable, executable, present, dirty, copy-on-write
   Это мета-информация о странице.
   readable      - можно ли сейчас страницу читать
   writeable     - можно ли сейчас в страницу писать
   present       - есть ли страница сейчас в памяти вообще (или надо лезть в своп, например)
   executable    - можно ли эту страницу исполнять (т. е. перекинуть на нее IP и читать инструкции)
   dirty         - модифицировалась ли страница
   copy-on-write - является ли страница copy-on-write (т. е. нужно ли в случае записи скопировать страницу (мало ли, вдруг она используется в нескольких процессах, как глобальные переменные некоторой shared либы, например)
** 3 IOMMU
   IOMMU - это MMU для периферических устройсв. Как мы знаем, периферические устройства могут общаться с памятью напрямую с помощью DMA, так вот, устройствам тоже хочется иметь свои виртуальные адреса (пример: шейдерам на видеокарте тоже надо разделять адресное пространство).
   IOMMU - это ровно MMU, только приставленный не к процессору, а к DMA-контроллеру.
   Кроме того, IOMMU еще и повышает секьюрность, так как на компе с физической DMA-адресацией хитровыебанное периферическое устройство может считать/затереть важные области памяти в обход CPU через DMA. С IOMMU такое не проканает.

   sources:
   [[https://en.wikipedia.org/wiki/IOMMU]]
** 2 Память процессов
   Память процесса устроена так:
   | .text | .data | .bss | Heap --> .... <-- Stack | argc, argv, env |
   0                                                                  N

   Но это в Линуксе примерно так, а вообще, конечно, везде по-разному.
   Во время переключения процесса (context switch) происходит, кроме всего прочего, очистка TLB и переключение page tables для процесса. Чтобы не копировать их туда-сюда, страницы 1 уровня всегда находятся в выделенной области памяти на своем месте, страницы нижних уровней раскиданы по памяти произвольно, и во время переключения контекста ядро переставляет указатели на page tables 2 уровня.

   sources:
   [[http://www.thegeekstuff.com/2012/03/linux-processes-memory-layout/]]
   [[http://unix.stackexchange.com/questions/72680/how-does-linux-update-page-table-after-context-switch]]
** 2 Общая память
   Sytem V style общей памяти.
   В Линуксе, например, можно выделять отдельные страницы памяти, доступные для нескольких процессов.
   В железе это выглядит очень просто - в page tables разных процессов есть ссылки на одну и ту же страницу физической памяти.
   В коде это делается системными функциями:

   shmget() -- выделить кусок общей памяти
   shmat()  -- присобачить этот кусок в виртуальную память процесса
   shmdt()  -- убрать кусок из виртуальной памяти процесса
   shmctl() -- освободить общую память
** 2 Пулы памяти со специальными требованиями.
   В ядре очень часто возникает задача создать/убить какой-то стандартный объект, такой, как inode/process descriptor/semaphore и т. д. Если делать это все время стандартным malloc-ом, это будет долго и грустно - маллок сам по себе долгий, и фрагментация памяти - это тоже не очень. Поэтому возникает идея: стандартные объекты ядра помещать в выделенные места.

   Slab - это как раз такое выделенное место - 1 или несколько последовательных страниц, предназначенных для размещения объектов с наперед заданным размером.
   Slab-кэш - это набор slab-ов, предназначенных для какого-либо типа объектов - inode там, вот это все.

   Аллокация объекта в slab-кэше происходит так:
   1) Выбирается первый slab со свободными "комнатами"
   2) Если все slab-ы заняты, маллочится новый.
   3) Свободная комната в выбранном slab-е помечается как занятая, и возвращается указатель на нее
   4) Если это была последняя комната, slab помечается как заполненный

   Удаление объекта:
   1) Комната объекта помечается как свободная
   2) Если это была последняя занятая комната, slab помечается как свободный

   sources:
   [[http://www.win.tue.nl/~aeb/linux/lk/lk-9.html]]
   [[https://ru.wikipedia.org/wiki/Slab]]
* 1 Ticket 4    Maps
** 2 "География" адресного пространства процесса
   Смотреть память процессов в билете 3
** 0 Структуры ядра описывающие процесс с MMU: работа с физической памятью, VMA

** 2 Системные вызовы: brk, sbrk, mmap
   Вершину хипа называют program break. Можно работать руками прямо с вершиной хипа, выделять память и вот это все.
   * brk(void* addr) выставляет вершину хипа куда-то.
   * void* sbrk(int) инкрементирует/декрементирует вершину хипа на заданную величину.
   По очевидным причинам, вызовами пользоваться не нужно. С другой стороны, иногда через это выражают malloc.

   Есть другой механизм гибкой работы с динамической памятью:
   mmap, numap, mlock, munlock - забавные системные вызовы.
   * mlock(void* addr, int len) -- заблокировать регион в памяти так, что ее страницы нельзя класть в своп.
     Юзкейс -- хотим загрузить программу для расширофки HDD с диска. Расшифровываем руками диск, грузим, выкидываем ключ, лочим программу в памяти и тогда она никуда не пропадет.
   * munlock(void* addr, int len) -- обратное действие, разлочвает регион если его часть залочена.
   * mmap(addr, length, prot, flags, fd, ofs) -- создает новый маппинг региона виртуальной памяти.
     1. addr -- откуда мапить память (если NULL, ядро само выберет)
     2. len -- длина
     3. prot -- флаги доступа к данному региону памяти (PROT_NONE, _EXEC, _READ, _WRITE)
     4. flags -- различные флаги. Самые важные из них:
        1. MAP_ANONYMOUS -- аргумент fd игнорируется, память создается анонимной, заполняется нулями, не относится ни к какому файлу.
        2. MAP_SHARED -- устанавливает, что контент, котрый мы мапим, будет доступен другим пользователям (мапится-то файл, а файл могут видеть другие).
        3. MAP_PRIVATE -- создает copy-on-write маппинг, который уникален для процесса.
     5. fd, ofs -- если используется маппинг файлов, а не анонимный, то файл и оффсет в файле, с которого мапить.
   * munmap(addr, length) -- освободить от мапа данный кусок памяти. Если используется SHARED, то к этому моменту все уже будет записано в файл.
** 2 Общая память: shm_open
   Есть память, которая может быть доступна сразу многим процессам.
   * shm_open(name, oflag, mode)
     Определяем имя куска памяти, которое хотим шарить, флаги на уровне RDWR/RDONLY/CREAT.., мод (chmod(2)). Возвращает файловый дескриптор.
   * shm_unlink(name) удаляет кусок расшаренной памяти
   С помощью mmap тоже можно создать общую память, но отличие shm_open в том, что память открытая этим методом будет оставаться шареной до ребута ядра.

   Как этим пользоваться:
   shm = shm_open("mysharedname", O_RDWR, 0777)
   addr = mmap(0, size, PROT_WRITE|PROT_READ, MAP_SHARED, shm, 0);
   Теперь по адресу addr можно что-то писать.
   munmap(addr, size);
   close(shm)
   Если хочется шареную память удалить, нужно сделать:
   shm_unlink("mysharedname");

   source:
   http://habrahabr.ru/post/122108/
** 1 Реализация malloc.
   1. На хипе с помощью sbrk
      Недостатки -- фрагментация данных. Чтобы от нее избавиться, нужно иногда искать куски памяти которые уже освободились и что-то с ними делать.
   2. С помощью mmap
   3. И то и другое использовать, плюс добавить какие-нибудь бакеты, из которых выдавать данные. Бакеты добавлять с помощью sbrk, mmap'ом пользоваться если просят много памяти.
   4. Doug Lea pmalloc2 (в libc)

   sources:
   1. Doug lea outdated explained
      http://gee.cs.oswego.edu/dl/html/malloc.html
   2. Wiki, implementations
      https://en.wikipedia.org/wiki/C_dynamic_memory_allocation#Implementations
   3. LIBC source pmalloc2 (extra hard)
      https://sourceware.org/git/?p=glibc.git;a=blob_plain;f=malloc/malloc.c;hb=HEAD
* 0 Ticket 5    Processes
** Процессы и треды
** init, родители, дети, зомби
** Треды, группы тредов, процессы, группы процессов, сессии
** Системные вызовы fork, clone, exec, wait
** Интерфейс bash
** Реализация переключения контекстов процессов: структуры данных ядра, состояния процессов, различные методы реализации CPS-преобразования.
* 2 Ticket 6    FS 0
** 2 Файловые дескрипторы и пайпы

   Файловый дескриптор - это int, который для текущего процесса сопоставляется с неким файлом. Сопоставление идет так:
   * fd - индекс в fdtable процесса. В ней лежит *FdObj - указатель на структурку, в которой хранятся флаги, количество сославшихся и ссылка на сам файл.
     FdObj = {
         counter :: Int,
         flags :: Flags,
         type :: *,
         resource :: type
     }
   * resource указывает на файл в file table (общая для системы таблица открытых файлов). Этот файл может быть структуркой какого-то типа: DInode, Pipe, Socket, etc.
     Каждая такая структурка так или иначе содержит в себе счетчик ссылок на себя из процессов и ссылку на свой inode.
   * Inode-ы, собственно, содержит адреса сегментов памяти, в которых расположен файл.

   Пайпы - это специальные двунаправленные файлы. Делаются они следующим образом: есть специальная виртуальная файловая система pipefs, которая лежит вообще в отдельном неймспейсе (нет такой папки в корне, которая бы ее содержала). В ней лежит inode, который маппится на какой-то кусок памяти. На него ссылается файл-пайп в file table, который вот такой:
   Pipe = {
       readers :: Int,
       writers :: Int,
       inode   :: *Inode
   } (примерно)
   В fdtable процесса на него ссылается 2 FdObj: с read-only и write-only

   Ну и короче в 1 конец пишем, другой читаем!))

   sources:
   https://en.wikipedia.org/wiki/File_descriptor
   [[http://www.slideshare.net/divyekapoor/linux-kernel-implementation-of-pipes-and-fifos]]
** 2 Системные вызовы open, read, write, close
   open(path, flags)   – открыть файл по данному пути и вернуть fd
   read(fd, buf, len)  - прочитать что-то из файла в буфер
   write(fd, buf, len) - записать чего-то туда
   close(fd)           - закрыть дескриптор (удалить его из fdtable и декрементнуть счетчик ссылок на файл)

   Все эти вызовы - абстракция VFS, а драйвера каких-то физических систем предоставляют им реализации, понятно.
** 1 Структуры данных ядра: таблица файловых дескрипторов, файловые объекты POSIX, флаг CLOEXEC
   Таблица файловых дескрипторов - смотри выше.
   Файловые объекты POSIX — эээм? как раз вот эти DInode, Pipe, Socket?
   Флаг CLOEXEC - удобный флаг, который заставляет fd закрываться всегда, когда успешно вызвана функция из семейства exec.
** 2 Системные вызовы dup2, fcntl, flock
   dup2(fd1, fd2) - дублировать дескриптор: fd2 начинает указывать на тот файл, на который указывает fd1. Если fd2 на что-то указывал раньше, он сначала закрывается.
   Удобно, когда надо перенаправить куда-то stdin или stdout.

   fcntl - йобо-функция, которая принимает кучу различных флагов и позволяет делать почти все, что могут делать прочие функции: ставить всевозможные флаги, лочить/анлочить, и еще дохера всякой ненужной и advanced херни. Также, в отличие от flock, ей можно делать lock только части файла.

   flock - залочить файл за процессом. Известная фигня - нам нужно, чтобы только мы писали/читали из файла/в файл. Лок бывает shared и exclusive - соответственно, "много процессов читают" и "один процесс читает и пишет". При попытке обращения к файлу прочие будут получать access denied, НО! Лок на файл не является строго обязательным. То есть, мало что мешает какому-то другому процессу что-то делать с залоченным файлом, если тот очень захочет.

* 0 Ticket 7    Drivers/ints
** Драйвера устройств в пространстве ядра
** Прерывания
** Монолитная обработка прерываний
** Hi/Lo прерывания
** Polling
** Драйвера как контексты исполнения и их отличия от процессов
** Реализация драйверов: структуры данных ядра, различные методы реализации CPS-преобразования.
* 0 Ticket 8    FS 2
** Файловые системы
** Структура данных inode
** VFS
   VFS -- такой стандартный интерфейс думать о разных файловых системах как об одной.
** Структуры данных: FSObject, Namespace
** Path resolution
** Операции над неймспейсами: mount, bind mount, move mount, chroot, pivot_root
** Linux FUSE
** ФС как функция inodeno → inode
** Структуры ядра: файловый дескриптор (на устройство, файл, директорию), различные кеши
** mmaping файлов.
* 0 Ticket 9    Users/access
** Пользователи и права
** Модели прав доступа к объектам: дискретная и ролевая
** Права на объекты файловой системы
** Пользователи и группы с точки зрения ядра
** Пользователи и группы с точки зрения пространства пользователя
** Системные вызовы setuid, setgid и товарищи
** setuid bit
** PAM
** /etc/passwd, /etc/shadow, /etc/group
** Capabilities.
* 0 Ticket 10   Signals
** Стандартные сигналы
** Маски
** Правила доставки
** Реалтаймовые сигналы
** Маски и очереди
** Правила доставки
** Системные вызовы kill, sigaction
** Прерывание сигналами: кода программы, обработчиков сигналов, системных вызовов
** Реентрабельность и безопасные системные вызовы
** Сигналы и треды
** Семантика сигналов: TERM, KILL, STOP, CONT, CHLD, PIPE, ILL/FPE, SEGV, BUS.
* 0 Ticket 11   Polling
** Мултиплексирование ввода-вывода
** O_NONBLOCK
** Edge и level triggered события
** Преобразование асинхронного ввода-вывода в синхронный CPS-преобразованием
** Структуры данных пространства ядра для реализации мультиплексора файловых дескрипторов
** Системные вызовы select, poll, epoll
** Управление скоростью передачи данных через файловые дескрипторы.
* 0 Ticket 12   Synchronisation
** Синхронизация
** Спинлоки
** Ядерные семафоры
** Блокировки и лизинги на файлы
** Структуры данных пространства ядра для реализации блокировок и лизингов
** 1 Системные вызовы: flock, fcntl.
   Смотри тикет 6
* 3 Ticket 13   Netstack
** 3 Сетевой стек
   Сетевой стек -- весь набор протоколов необходимых для соединения компюьтеров друг к другу.
   OSI -- один из первых базовых сетевых стеков, который не допилили. BSD сокеты -- протокол, который развивался параллельно. Проблема OSI с их разработкой была в планировке, к примеру часть разработчиков хотела разрабатывать пакетную передачу данных (на тот момент новшество), но разработчики из бывшых телефонных компаниях считали эту затею плохой. До пакетной передачи соединение обеспечивалось правильным подключением клиентов друг к другу через свичи.
   Существует три широко используемых протокола низшего уровня: MAC(через Ethernet/DSL), PPP, ARP.
   Первый всем хорошо известен, второй -- это как раз телефоны, третий используется для общения между устройствами в сети для обмена данными необходимыми для других протоколов (IP → MAC).
** 3 PPP
   Point-to-Point Protocol -- протокол, который применяется для соединения двух нодов. Физически используется в телефонии (как стандартной, так и сотовой), оптических сетях и где угодно. Есть два протокола, похожие на PPP: PPPoE, PPPoA -- over Ethernet/over ATM, использующиеся в основном провайдерами чтобы настраивать доступ клиента к внешнему миру.
   PPP пользуется протоколом LCP (Link Control) для того, чтобы установить сессию между узлами (например, пользователем и провайдером).
   PPP поддерживает 3 типа аутентификация для разной защиты. PAP (Password Authentication Protocol) -- протокол для аутентификации пользоватьского пароля на выделенном сервере. Это самый менее секьюрный вариант, пользователь просто отправляет пароль на сервер, а сервер его верифицирует. Ничего не шифруется.
   Есть CHAP (Challenge Handshake Authentication), который отсылает challenge message клиенту на машину, содержащию какую-то рандомную чушь. Машина клиента зашифровывает это вместе с паролем и отправляет обратно на сервер, который это отправляет на сервер аутентификации. Последний зашифровывает challenge с пользовательским паролем с проверяет наличие в базе. Используется модель shared secret, чтобы аутентифицировать пользователя.
   Самый секьюрный вариант -- EAP, про него много.
   PPPoE -- протокол уровня Ethernet, представляющий виртуальное соединение по PPP. Используется провайдерами, как уже было сказано. PPPoE discovery, процесс, который соединяет машины, выглядит так:
   1. Initiation -- клиент высылает специальный пакет на сервер
   2. Offer -- сервер отвечает другим похожим пакетом
   3. Request -- клиент на основании Offer создает пакет и отсылает на сервер
   4. Confirmation -- сервер понимает, что клиент живой, выдает уникальный ID клиенту для PPP сессии и высылает подтверждение клиенту.
** 3 Ethernet
   Самый простой сетевой порт это COM. У него 6 проводов, одна пара из которых передает данные в одну сторону, пара в другую, еще пара сигнальная. Двухпроводное решение широко используется, есть однопроводные, которые очень сложные и непопулярные (чипы в домофонах!). После установления соединения, по проводу передается меандр. Проблемы COM-порта в нарушении целостности сигнала из-за наводки.
   Ethernet реализован чаще всего в формате UTP (unshielded twisted pair) -- много маленьких проводов очень сильно скручены друг между другом. Сигнал по витой паре задается разницой потенциалов (по 2м соседним проводам передаются сигналы, а смысл имеет их разница). Такое решение имеет смысл, потому что любая наводка одинаково искажает сигнал на двух проводах и разность остается неизменной. UTP чаще всего в этой стране встречается в формате двух и четырех пар проводов. Проводов для синхронизации нету, SIGHUB генерируется по таймауту.
   Еще есть коаксиальный кабель (тонкий кабель внутри обертки), профит которого в том, что обертка как-то защищает внутренний кабель от помех, выступая некой клеткой Фарадея.
   Сеть по формату Ethernet до 1Гб/c реализуется подключением всех юзеров к одной общей ethernet-шине. Политика такая: пока кто-то посылает пакеты, другие молчат. Если есть несколько пользователей, которые пользуются шиной одновременно, все замолкают на рандомный интервал времени, потом продолжают. [[https://en.wikipedia.org/wiki/Carrier_sense_multiple_access_with_collision_detection][CSMA/CD]]
   Сеть с большей скоростью нуждается в различных вспомогательных машинах, типа хабов.
   Хаб -- железяка, которая передает пакеты, которые ему приходят, на все свои выходы (броадкастит).
   Есть еще параллельный COM для принтеров, называется LBT. Параллельные шины плохи, потому что сложно синхронизировать передачу данных по многим проводам одновременно, учитывая всякие помехи.

   Ethernet пакет по стандарту (802.3) состоит из:
   1. Преамбула (какие-то метаданные)
   2. Header
      1. Size of packet (or data). MTU -- 1500.
      2. Два MAC-адреса (отправитель и получатель)
      3. Протокол (ethertype) -- есть разные форматы ethernet (LLC, Ethernet II)
   3. Payload (всякие данные)
   4. CRC (контрольные суммы) всего кроме данных
   Больше здесь: [[https://en.wikipedia.org/wiki/Ethernet_frame][Ethernet Frame]].

   Wifi представляет из себя Ethernet по радио. Есть некоторый диапазон частот разрешенный для использования wifi-устройствами, который разделен на поддиапазоны -- каналы. В разных странах используются разные диапазоны. Wifi передает синусоиду, все как по радио, с этими вашими модуляциями.
** 3 IP
   IP протокол находится на уровне выше и обычно запихивается в Ethernet. IP пакет содержит флаги, IP адресата/адресанта, данные и crc для всего кроме данных.
   Сетевые карты обычно фильтруют пакеты которые ей не принадлежат (адрес назначения не совпадает с нашим) на уровне MAC, IP пакеты фильтрует уже ОС. И то и другое поведение может быть изменено с помощью [[https://en.wikipedia.org/wiki/Promiscuous_mode][promiscuous mode]] опции.
   Ethernet bonding — это объединение двух или более физических сетевых интерфейсов в один виртуальный для обеспечения отказоустойчивости и повышения пропускной способности. Гуглится.

   IP -- уникальный идентификатор размером в 4 байта. Подсети бывают классов A, B и C. Для класса A определена маска 255.0.0.0, для B 255.255.0.0, для C 255.255.255.0. Кроме того, определены зарезервированные адреса для сетей: A: 10.0.0.0, B: 172.16.0.0 -- 172.13.0.0, C: 192.168.0.0 -- 192.168.255.0. Маска подсети -- число от 0 до 32, означающее количество единичек перед ноликами в двоичной записи 4-байтового числа. Маска записывается как IP. 24 -- 255.255.255.0, 31 -- 255.255.255.255. Первая нотация называется префиксной (CIDR).
   Работает это следующим образом. Пусть нужно отправить пакет. У каждого интерфейса в компьютере есть своя маска и IP (ifconfig -a). ОС выбирает интерфейс, который наиболее близок по маске с ip с ip адресата (сравниваются and, полагаю). В BSD сокетах это поведение реализуется, если делать bind(0.0.0.0). Можно сделать bind на конкретный интерфейс, и тогда пакеты будут отправляться ровно куда надо.
   Ядро хранит таблицу роутинга, которая говорит, какие пакеты в какой интерфейс пихать (ip r, netstat -rn, route). Есть дефолтный гейтвей (шлюз по умолчанию), в который отправляются пакеты, если они не матчатся по другим маскам (default в route).
** 3 ARP
   Проблема отправки Ethernet-пакетов состоит в том, что нам нужны MAC-адреса (можно указать MAC broadcast ff:ff:ff:ff:ff:ff). Что делать, если у нас есть только IP?
   Если IP адрес не лежит в нашей локальной сети, то все просто - мы знаем MAC-адрес роутера, Ethernet-пакет дойдет до него, роутер его распакует и отправит дальше куда нужно. Что, если роутера нет - у нас простая локальная сеть?
   Для того, чтобы по IP найти MAC, существует протокол ARP. Ядро содержит ARP-таблицу, которая заполняется по мере необходимости и отображает IP в MAC (arp -e). Если в таблице нет записи, а нужно отправить, по сети прогоняется ARP-запрос на уровне "у кого тут такой ip?", и получает ответ.
   Обратный протокол получения IP по MAC первоначально назывался RARP (reversed ARP). Потом он перетек в BOOTP, теперь это DHCP. Существенная разница RARP и DHCP в том, что DHCP -- протокол на уровне TCP/IP, а RARP был на netlink уровне (самом низком). Зачем DHCP оборачивать в IP -- никто не знает.
** 3 Hardware
   Напомним: хаб -- железяка, которая передает пакеты, которые ему приходят, на все свои выходы (броадкастит).
   Свитч -- это хаб с ARP таблицей внутри, который умеет отправлять пакеты не всем сразу (как хаб), а только тем, кому надо, если в ARP-таблице есть необходимая запись.
   Маршрутизатор -- это свитч с таблицей маршрутизации! Конечно, он тоже имеет ARP, и чаще всего связывает локальную сеть с внешним миром. В таком случае, обычно, в локальной сети у нодов дефолтный гейтвей как раз машрутизатор. Сам маршрутизатор получает свой дефолтный гейтвей обычно от провайдера.
** 3 TCP/UDP/SCTP
   Протоколы, которые обычно запихивают в IP: UDP, TCP, SCIP
   1. UDP, TCP: хедеры, в UDP crc берется от хедеров, в TCP от всего пакета. UDP не обеспечивает никакого механизма проеврки доставки пакета, в отличии от TCP. TCP/UDP пакеты внутри содержат порт (/etc/services).
      Механизм подключения в TCP похож на трехкратное рукопожатие:
      1. Отправляется запрос 1→2 (syn)
      2. Отправляется подтверждение о получении запроса 2→1 (syn-ack, ack = acknowledgment), эта сторона запоминает кому отправила syn-ack
      3. Клиент отправляет 1→2 (ack) еще раз и сервер проверяет, правда ли, что отправлял клиенту syn-ack. Если да, соединение установлено.
      Забавное наблюдение заключается в том, что можно много раз отправлять некоторому набору серверов syn с подмененным ip возврата, и syn-ack будут возвращаться на желаемый адрес, от чего желаемому адресу может стать плохо. Еще минус -- приходится хранить в сервере данные о том, кому отправил syn-ack.
   2. SCTP (stream control transport protocol) -- штука похожая на TCP, но если среди N пакетов некоторые зафейлились, то только зафейленные будут отправляться заново (в TCP все начиная с первого зафейленного). Кроме того, этот протокол подразумевает, что всякие данные для подключения отправляются клиенту от сервера зашифрованными и только сервер может их расшифровать, когда эти же данные ему придут в ack. Отпадает необходимость помнить о syn-ack которые сервер отправляет.

   IPv6 имеет все из коробки внутри. Имеет обратную совместимость с IPv4, зашитый внутрь MAC. Утверждается, что использование IPv6 избавляет от необходимости использовать NAT и DHCP.
** 2 BSD sockets: API, Stream-сокеты, Datagram-сокеты, RAW-сокеты, файловый объект для accept-сокета.
   CLOSED: [2015-06-29 Mon 13:14]
   man socket
   BSD socket API выглядит примерно так (по всему лучше читать man):
   * socket(...) -- создать сокет. Тут устанавливаются всякие параметры, тип сокета (datagram -- UDP, stream -- TCP), другие настройки.
   * connect(...) -- создать соединение на сокете. Первоначально сокет висит в пространстве и ничего не делает, connect его инициализирует.
   * bind(...) -- другой способ инициализации сокета, серверный.
   * listen(...) -- обычно следует за bind.
   * getaddrinfo(...) -- супер обобщенный вызов, возвращающий данные о хосте, которые могут быть использованы для создания сокетов. Прелесть в том, что он удобный и одинаковый для ipv4/v6 сокетов (и еще много чего).
   * Далее с сокетами можно обращаться с помощью read/write, но есть специальные вызовы: send/recv, sendto/recvfrom, sendmsg/recvmsg. Все они -- это write/read со специальными флагами + можно передавать какие-нибдуь допопции + падает если соединения нет + еще перделки.

   Есть файл /etc/nsswitch.conf. Сервисы типа getaddrinfo пользуются им чтобы определить откуда искать данные. К примеру, в nsswitch поле hosts хранит "files dns", что соответствует /etc/host.conf и /etc/resolv.conf. Есть демон nscd, который занимается тем, что резолвит запросы "откуда мне бы почитать". Этот демон первоначально запускается от рута и как-то связан с ldap, может резолвить пароли. Есть еще PAM, которой все пользуются (su), и иногда эти сервисы могут конфликтить.

   man socket описывает семейства сокетов как IPv4, IPv6, полезно еще знать про существование AF_UNIX, который используется для общения ядра самого с собой.
   Сокет конкретного семейства имеет тип. RAW сокеты -- это уровень IPv4, но сырой, без части хедеров. Поскольку с такими сокетами можно набагать и застопорить какую-нибудь очередь IO, они доступны только руту.

** 3 ICMP, TFTP, DNS, NAT
   ICMP протокол, который завернут в Ethernet, используется для общения между роутерами, логирования ошибок, для ping/traceroute. IP пакеты имеют TTL и на каждом hop отправляют запрос обратно.
   TFTP -- UDP-протокол, обеспечивающий наивную реализацию того, что делает FTP (достань-ка мне тот файл).
   DNS -- /etc/resolv.conf. Та самая штука, которая мапит имена в <host,port>. Самый простой вариант использовать DNS -- gethostbyname -- как раз получает IP по хосту. DNS пакеты имеют тип и имя. Типы: A(IPv4), AAAA(IPv4), MX(email), TXT(что угодно). Именем является хост. Ответы бывают рекурсивными и нет. Рекурсивные ответы возвращают кучу ip-адресов, соответсвтующих одному хосту (например, сервер распределяется между несколькими хостами для уменьшения нагрузки).
   NAT (network address translation): пусть есть локальная сеть и мы пользуемся внутри локальной адресацией. Тогда если узел отправляет пакет во внешний мир, он проходит через шлюз по умолчанию. Устройство, которое имеет адрес шлюза, содержит таблицу, которая сохраняет данные о пакетах. Устройство подменяет source пакета на свой, и отправляет куда надо. Когда возвращается ответ, он перенаправляется юзеру согласно таблице. Существуют хаки, которые позволяют отправлять пакеты напрямую. Гуглить tsocks, UPnP.
* 3 Ticket 14   Terminals/Groups
** 3 Терминалы, псевдотерминалы и режимы их работы
   Как это было раньше:
   1. Line discipling -- это набор правил для обработки текста.
      Различают два мода line discipling:
      1. raw -- приложение обрабатывает все сигналы с клавиатуры, которые ей приходят
         (так делает vim/emacs/...)
      2. cooked -- приложение получает данные построчно, причем обработкой строчек перед отправлением (редактирование) занимается ОС.
   2. TTY driver -- это драйвер, который занимается кучей разных вещей, в том числе определяет понятие бэкграундных процессов и основного, возможности их останавливать и запускать в разных режимах.
   3. UART (Universal Asynchronous Receiver and Transmitter) -- это драйвер операционной системы, который занимается физическим транслированием байтов, контролем четности битов (parity check) и прочее.

   TTY -- это тройка из <Line discipline, TTY, UART>.
   Как работает TTY в древник пека:

   Hardware.......................   Kernel..........................................   Userspace......
   Terminal---Physical line---UART---[--UART Driver---Line Discipline---TTY Driver--]---{User Processes}


   Как это работает в POSIX:
   У нас нет никакого UART. Нет никакого физического терминала, вместо этого есть видео терминал -- эмулирующаяся штука, которая содержит framebuffer -- виртуальное или реальное устройство, которое хранит битмапы и умеет выводить их на экран. И этот видео терминал рендерится в VGA дисплей.

   Hardware.....  Kernel......................................................    Userspace.......
   Display<-------VGA Driver<----┐
                               Terminal Emulator--Line Discipline--TTY driver----{User Processes}
   Keyboard--->Keyboard driver->-┘

   Чтобы облегчить себе жизнь, были созданы псевдотерминалы -- pty(7). PTY -- это пара псевдоустройств, одно из которых (slave) эмулирует текстовый терминал. Когда к терминалу хочет подлючиться какая-то программа, то она будет управляться той программой, которая открыла другой конец (master).
   Вот как работает утилита script(1): [[https://upload.wikimedia.org/wikipedia/commons/e/ef/Termios-script-diagram.png][script-diagram]]
   Она фактически запускает внутри себя баш, который посылает все наверх, а в это время script пишет всякий инпут/аутпут в файл.

   sources:
   TTY unmistyfied (очень советую, просто супер забавная статья)
   http://www.linusakesson.net/programming/tty/index.php
** 3 Группы процессов, сессии, управление заданиями
   Процесс может быть в одном из пяти состояний:
   * R -- запущен или может быть запущен
   * D -- ждет какого-то евента в непрерываемом сне
   * S -- прерываемый сон (ждем евента или сигнала)
   * T -- остановлен либо по контрольному сигналу либо дебаггером
   * Z -- зомби, закончившийся, но о котором забыл родитель (не сделал wait(2))

          ┌--→S---┬-------┐
          ↓       ↓       ↓
   D ←--→ R ←---→ T ←---→ Z
          ∣               ↑
          └---------------┘

   ps 1 выводит запущенные/спящие процессы и чего они ждут. Более того, в графе STAT может быть флажок s -- этот процесс лидер сессии.
   Управление заданиями -- это все действия с процессами, которые касаются откладывания на задний фон, суспендинга, и прочих похожих вещей.
   Сессии и задания -- это разные уровни объединения процессов в группы. Выгляит это так:
   {Процессы} ⊂ Группа
   {Группа} ⊂ Сессия
   У каждой группы есть лидер группы (process leader), у сессии есть лидер сессии (session leader). Сессия соответствует некоторому терминалу,
   (Примечание: на самом деле есть tid -- thread id, уровень ниже процесса, треды объединяются в процессы (thread group), и у таких групп есть лидеры, при закрытии которого треды тоже умирают).
   Логика такова: fork создает процесс в своей группе, сессия менеджит сигналы. Bash понимает что пришел, например, ^C и отправляет его текущей fg группе, от чего умирают все pid'ы в ней. Шелл является лидером сессии и поэтому каждый новый запуск чего-нибудь создает новую группу.

   Понятно как делать foreground/background процессы если ты шелл. Типа просто создаешь с помощью некоторого системного вызова (полагаю, что clone) новую группу и чилда, и запускаешь, либо связывая текущий stdin/out с запущенным чилдом, либо отмечаешь, что они живут и работаешь дальше с юзером. fg/bg -- утилиты, которые могут взять последнюю использованную группу процессов и отправить ее в bg/fg.
** 3 Сигналы: INT, HUP, TSTP, TTIN, TTOU, WINCH
   Поскольку в UNIX все tty/pty файлы, то ими можно управлять с помощью классного ioctl(2) -- швейцарского ножа UNIX относительно девайсов. Есть некоторая загадочная проблема с асинхронностью работы ядра с приложением, если юзать в приложении вызовы к ioctl. Поэтому на помощь приходят сигналы.

   Полный список сигналов есть в man, мы рассмотрим следующие:
   1. SIGHUP
      Default action: Terminate
      Possible actions: Terminate, Ignore, Function call
      Отсылается драйвером UART к сессии целиком, когда мы замечаем зависание на девайсе. По дефолту оно убивает все процессы. Тем не менее, программы типа nohup(1) и screen(1) отключаются от сессии, так что их процессы не заметят SIGHUP.
   2. SIGINT
      Default action: Terminate
      Possible actions: Terminate, Ignore, Function call
      Отпавляется драйвером TTY конкретной forground job обычно по ^C (пока это поведение не выключить с помощью stty). По дефолту SIGINT аффектит группу и убивает ее сразу (напомним, что в шелл группа -- это какая-нибудь пайпнутая последовательность команд или любая конкретная команда, которая была запущена, а также все ее чилды).
   3. SIGQUIT
      Default action: Core dump
      Possible actions: Core dump, Ignore, Function call
      SIGQUIT -- это SIGINT по ^\, который имеет немного другое дефолтное действие и помогает, когда программа нагло игнорирует SIGINT (если конечно не стоит хендлер и на QUIT).
   4. SIGSTOP
      Default action: Suspend
      Possible actions: Suspend
      Очень модный сигнал, который нельзя превентить и замаскировать. Обычно SIGSTOP не вызывается напрямую. По ^Z отправляется SIGSTP, а потом само приложение себе посылает SIGSTOP.
   5. SIGTSTP
      Default action: Suspend
      Possible actions: Suspend, Ignore, Function call
      Работает как INT/QUIT, но магическая кнопка -- ^Z и дефолтное действие -- остановить.
   6. SIGCONT
      Default action: Wake up
      Possible actions: Wake up, Wake up + Function call
      Выводит процесс из состояния сна. Он посылается, когда юзер вызывает fg.
   7. SIGTSTP
      Еще один сигнал (наравне с SIGSTOP и SIGSTP), суть которого в том же самом и большинство источников утверждает, что именно этот сигнал посылается по ^Z.
   8. SIGTTIN
      Default action: Suspend
      Possible actions: Suspend, Ignore, Function call
      Если бэкграундный процесс пытается читать из TTY, этот сигнал посылается, чтобы та задача засуспендилась.
   9. SIGTTOU
      Default action: Suspend
      Possible actions: Suspend, Ignore, Function call
      Если бэкграундный процесс пытается писать в TTY, ему посылается SIGTTOU, который засуспендит задачу.
   10. SIGWINCH
       Default action: Ignore
       Possible actions: Ignore, Function call
       TTY девайс следит за параметрами терминала, и эта информация должна периодически обновляться. Если размер изменился, TTY посылает SIGWITCH foreground'ной задаче. Всякие редакторы должны корректно растягиваться и перерисоваться.
** 3 Демоны и демонизация
   Демоны -- это такие задачи, которые висят в фоне и не аффектят другие процессы напрямую. Демоны не принадлежат группам и не привязаны к TTY. Обычно демонизируют всякие серверы и сервисы.
   Чтобы стать демоном, нужно:
   1. Закрыть stdin/stdout/stderr
   2. Отсоединиться от TTY
      В этом две сути -- демон и все его потомки не смогут открыть терминал и что-то испортить. Плюс, демоны не будут зависимы от HUP, когда юзер выходит из shell после запуска сервера.

   Хорошей практикой для сетевых демонов есть:
   1. Поменять директорию на /
   2. Поменять собственную маску создания файлов (chmod)
   3. Нормализовать PATH
   4. Записать свой pid в /var/run или еще куда-нибудь
   5. Дополнительно, настроить логирование
   6. Дополнительно, сделать chroot в какой-нибудь удобный environment, где ничего нельзя испортить.
* 2 Ticket 15   Booting
** 2 Pre-BIOS: хардварная загрузка
   Материнская плата имеет огромное число всяких разных защелок, кнопка включения/reset приводит их в детерменированное состояние. Затем подается питание на процессор и указатель направлен в константную память, в которой лежит BIOS.
   Тут Ян минут 5-10 рассказывал, но вроде не критично и не нужно.
** 3 Загрузка: BIOS → MBR (DOS Label), DOS/Windows boot, GRUB
   Первым делом BIOS инициализируется. Затем иницализируется VGA BIOS -- штука которая инициализирует VGA-контроллер. Происходит проверка системы на целостность, прогоняются тесты. Существуют различные вариации селф-тестов, в зависимости от желаемого времени прохождения. Первоначально целью этих тестов было получить размер оперативной памяти (программа подсчитывала количество байтов линейно).
   Далее проверяется наличие всех необходимых контроллеров. Ровно тут существовала популярная ошибка "no keyboard detected" -- старые операционные системы не могли работать без клавиатуры. Более того, раньше никто не задумывался о необходимости запускать ПК без клавиатуры или видеокарты, так как самый популярные юзкейс -- серверы, а раньше из обычных ПК серверы никто не делал, там железо требовалось особое. Кстати говоря, клавиатуры подключались через PS/2 -- он очень простой, в этом профит.
   Если у BIOS происходят какие-то ошибки, понять, какие конкретно, сейчас можно по специфическим гудкам, которые он издает с помощью встроенного динамика (и документации). BIOS можно дампить, он там свой стейт как-то в CMOS хранит.
   После прохождения self-тестов BIOS предоставляет возможность что-то сделать, войти в какой-нибудь GUI по нажатию f11, например. Раньше кастомизация BIOS происходила с помощью джамперов, которые выставлялись один раз перед загрузкой.
   Затем происходит загрузка с дефолтного boot устройства. Тут необходимо посвятить время основной загрузке с жесткого диска и немного сетевой загрузке.

   При сетевой загрузке используется PXE. У нас есть сетевая карта, драйвер к которой давным-давно был расположен на самой кате, а сейчас он есть в BIOS. BIOS может с ее помощью вытаскивать необходимые данные. Есть несколько вариантов PXE на данный момент, самый популярный -- pxelinux, или ipxe. Первый поддерживает TCP, и это очень круто, потому что TFTP по UDP может терять пакеты и если ядро большое, можно много раз безуспешно пробовать его загружать. Частая практика с PXE использовать chained requests. Один PXE вытаскивает с сервера некоторый код, который предоставляет GUI для того, чтобы выбрать другой удаленный сервер и выбрать ядро, которое тебе нравится (например).

   Стандартная загрузка с жесткого диска происходит следующим образом:
   BIOS загружает необходимые ему драйвера: ATA-IDE, SATA-SCSI, USB. USB драйвера труднее писать из-за того, что USB необходимо постоянно поллить (значит ли это, что драйвера для USB не всегда включены в BIOS?). Затем BIOS грузит в оперативную память первые 512б и загружается с них. Есть несколько вариантов разметки жесткого диска, которые позволяют делать разные приятные вещи, к примеру MBR или GPT. Отметим, что нет ничего противозаконного загружаться напрямую из какого-то кода (как grub или ваш_кастомный_загрузчик).
   MBR (DOS label) формат: 512b. Последние два байта это 0x55AA, необходимы для первоначальной проверки того, что шина работает. Кроме того, это индицирует, что диск размечен MBR а не чем-то другим. Первые 510б -- это jump, метаданные, загрузчик и TBL. Jump просто перепрыгивает метаданные. Все, что делает код -- загружает нужный кусок памяти и запускает его.
   TBL (загрузочная таблица) содержит 4 основных (primary) раздела, для каждого определено boot bit, тип, старт (адрес) и длина. Boot bit показывает, что с этого раздела нужно грузиться. Кроме 4 основных разделов можно добавить еще некоторые дополнительные (extended).
   Любой раздел кроме уже перечисленного содержит (в таблице) свой тип и другие данные. Linux игнорирует TBL-информацию о типе, но для DOS это критично. Обычно ядро лежит по фиксированному адресу на диске. Важное замечание: fdisk не дает создать раздел раньше чем некоторый оффсет с начала диска по причине того, что начало обычно резервируется для MBR + еще потенциально чего-нибудь. Кроме того, даже начало MBR не совпадает с началом диска, а есть еще оффсет, который свойственен для конкретной модели HDD ввиду того, что дорожки близко к центру плохо отцентрованы.
   GRUB обычно устанавливается как раз сразу за MBR и занимает секцию кода в MBR. Также GRUB содержит рядом со своим исполняемым кодом различные драйвера. GRUB похож на маленькую OS, которая загружает разделы с помощью драйверов которые вот там есть и показывает GUI, предоставляя возможность настраивать все, что настраивается.
   Забавный факт: с некоторых пор GRUB начал хранить себя еще и с crc, по причине того, что Windows никак не защищает этот кусок памяти, и туда могут благополучно писать кто хочет, в том числе и Photoshop, который хранит где-то в этом месте свои ключи регистрации, чтобы пользователи после переустановки системы не могли сбросить лицензию.

   С GPT памяти на то же, что использовалось в MBR, намного больше. В начале есть MBR-совместимая таблица, потом располагаются 512 ячеек TBL.
   Утверждается, что с помощью BIOS без внешнего загрузчика нельзя загружать что либо в файловой системе, то есть единственный вариант -- ядро класть прямо по адресу.
   EFI -- это такой BIOS, который пишет в NVRAM все загрузочные данные, а конкретно откуда и с каким оффсетом лежат ядра, всякие аргументы и прочее.
** 3 initrd
   Initrd -- это cpio архив, который грузится в память, а ядро затем монтирует это как дефолтную систему. Основная цель initrd -- обеспечить дополнительную функциональность, когда слишком сложно/лень писать новый модуль ядра. Initramfs -- это модификация initrd, доступная в linux с 2.6.13, которая монтируется как tmpfs.
   Пусть у нас есть ядро и initrd. Ядро обычно находится по некоторому фиксированному адресу. Ядру передаются параметры, в том числе root={dev|UUID}, который говорит, что монтировать в корень (blkid). Ядро обычно сжато bzImage, с тех пор, как оно стало достаточно большим -- это довольно специфичный архивный формат (не связан с bzip2), основанный на gzip. В начале этого архива есть программа для разархивации.
   GRUB запускает ядро, распаковывает initrd и монтирует его в корень (/). Затем запускается /init, который дает старт загрузке. Всякие встроенные устройства типа роутеров как раз имеют ровно ядро и initrd, которые лежат в некоторой NVRAM. В таких встроенных системах пользуется популярностью busybox -- программа, которая имитирует стандартный набор утилит linux (парсит 0 аргумент и запускает что надо). В случае, если установлен busybox, /bin/{ls, mv, cp, cat} -- symlink'и на busybox (busybox --help, busybox --install -s dir -- устанавливает симлинки на себя). Цель busybox -- иметь кучу всего, при этом не тратя много памяти (стандартные утилиты имеют достаточно ограниченный функционал, меньший, чем оригиналы).
   После этого создаются /dev/{stdin, stdout, stderr, console} и console выставляется на все стандартные std... (exec < /dev/console && exec > /dev/console && exec 2> /dev/console). Следующим шагом возникает необходимость примонтировать какую-нибудь файловую систему, и это делают двумя способами:
   1. mount -t procfd ... /proc, mount -t sysfs ... /sys; launch udev.
   2. Монтируется некоторая специфическая система (Ян не вспомнил названия), которая имеет udev внутри и создает все inod'ы автоматически.

   udev(7) -- это демон, который создает netlink (socket(2)) сокет с ядром, в которое ядро дампит информацию про устройства, а потом парсит эти данные, классифицирует (connect/disconnect/modify) и согласно правилам в /etc/udev что-то делает (чаще всего создает что-то в /dev, переименовывает или меняет симлинки). Существует также поведение udev, которое называется settle (udevadm(8)) -- udev обрабатывает всю очередь событий и выходит.

   После этого можно отмонтировать себя и загрузить желаемый раздел (а между тем что-нибудь еще расшифровать или сделать еще что-нибудь интересное, что позволяет initrd). Если используется busybox, то определить файловую систему помогает blkid, если нет, то полноценный mount сам может. Есть еще проблема с инициализацией SATA, так что blkid умеет ждать в цикле инициализации. Альтернативный подход к решению проблемы -- libsata модуль, которым никто не пользуется, потому что никому не нужен модуль, который ждет SATA и блокирует систему.
   Теперь мы можем примонтировать / и запустить init. Сделать это можно с помощью pivot_root && exec /sbin/init. Внутренний init делает что-то специфическое, свойственное для системы. В этом месте как раз мы расшифровываем диск, если он зашифрован. Можно тут загрузить вместо init просто emacs, который умеет делать сам практически все необходимое, тогда в /bin кроме него нужно положить еще mount, а busybox'а с головой должно хватить (тут много шутят про emacs OS still needs a better editor -- возьмитв evil/viper/vimpulse с собой!).

   Рассмотрим пример с USB:
   1. USB воткнут в порт.
   2. Проходит 300мс, необходимые для того, чтобы убедиться, что USB всунут плотно (лол).
   3. Контроллер на флешке понимает, что он подключился куда-то и отправляет сигнал
   4. Проходит через южный мост
   5. Через северный
   6. В процессор, который получает прерывание
   7. Ядро обрабатывает прерывание, смотрит на контроллер, понимает какой драйвер нужен
   8. Смотрит в табличку специальную, осознает какой модуль за это отвечает (если в ядре нету драйвера)
   9. Hotplugging: в ядро загружается код, который представляет из себя нужный модуль (код мерджится с кодом ядра), с зависимостями. Или все падает, если чего-то нет, хотя обычно утилиты конфигурации ядра (menuconfig в gentoo) такого не допускают, компилируя все зависимости.
   10. создается sys/..., в дело вступает udev и создает /dev/sdd{..}
   11. Все последующие прерывания обрабатываются уже из загруженного модуля.

   Все операции во время hotplugging'а происходят с помощью UNIX сокетов, которые должны быть вкомпилированы в ядро, иначе мы получим бесконечный цикл попыток загрузить модуль с UNIX сокетами.
   Все модули загружаются автоматически, но иногда приходится делать это вручную. Например, с помощью /etc/init.d/modules.
   Firmware загружается напрямую в железо (как например драйвер видеокарточки), и затем появляется возможность общаться с картой через стандартный интерфейс (opengl какой-нибудь).
** 3 Инициализация системы: последовательная, учитывая зависимости, resource/socket activation, lazy activation, cтандартные init системы
   Последовательная инициализация -- запустить все (!!!) последовательно! С зависимостями -- имеется некоторый набор сервисов, которые превращаются в граф. Resource activation -- не запускать сервер, пока не будет в том необходимости, то есть клиенты раньше сервера. Lazy activation -- полагаю, что приоритеты, как в systemd.
   Init системы:
   Первоначально был System V init -- демон, который создавал /dev/initctl сокет при старте. Можно отправлять в этот сокет команды запускать runlevel'ы (определены в /etc/inittab). Типа ты запускаешь runlevel с помощью rc N команды, и тогда rc запускает /etc/rc<N>.d/*K kill, потом /etc/rcN.d/*S start, смена runlevel'а останавливает все предыдущие процессы. Дефолтно 0 -- halt, 1 -- single user, 2 -- многопользовательский без сети, 3 -- многопользовательский с сетью, 6 -- reboot.
   System V init вполне себе ОК, когда задач не очень много, так что его используют на всяких встроенных системах, читалках и тд. Вторая проблема -- это демоны, их трудно трекать.
   Перед systemd, arch linux пользовался какой-то модификацией systemv с поддержкой асинхронного запуска программ.
   Первое нормальное решение вместо system v -- это Upstart, штука очень похожая на system v, но умеющая трекать демонов и мультизадачная, на основе событий -- некоторые скрипты создают евенты, которые другие события слушают, так что можно запускать что-то асинхронно. На события можно подписываться.
   Socket activation -- решение создавать все сокеты и каналы перед выполнением задач, а потом все сразу запустить. Такая штука использовалась некогда в Mac OS.
   Systemd -- init с поддержкой мультизадачности на графе сервисов, ребра которых либо сокеты, либо просто непосредственный запуск ресурса. Это более оптимально чем socket activation из-за того, что вершины имеют приоритет + проблемы с демонами решены с помощью механизма cgroups. Cgroups представляет собой набор процессов, объединенных круче, чем обычные группы процессов, а именно: можно ограничить группе доступ к памяти, дать группам разный приоритет по отношении к CPU/IO, можно убивать, чекпоинтить и рестартить всю группу сразу. Проблема демонов решена ровно потому, что из cgroup нельзя просто так выйти.
   Openrc -- gentoo init, который очень похож на systemd, но без безумных идей.
   # Тут нужно сказать почему systemd и kdbus -- очень ему не нравятся
** 3 Стандартные демоны: init, syslog, klog, cron, at, ssh
   1. init daemon -- уже обсудили
   2. syslog - демон, который читает из ядра логи и пишет их
      Поговаривают, он читает /dev/log сокет и делает что-то согласно /etc/syslog.conf
      http://www.k-max.name/linux/syslogd-and-logrotate/
      На gentoo все пользутся syslog-ng или rsyslog, которые умеют делать что-то с логами согласно конфигурации -- класть их в /var/log или пересылать по сети.
      https://wiki.gentoo.org/wiki/Rsyslog
   3. klog
      Тоже логгер какой-то видимо, про него ничего нету в интернетах
   4. cron
      Супер-полезная штука, которая делает какие-то вещи по расписанию, будь то бекапы или обновления, или еще что угодно
   5. at
      Демон atd висит и исполняет команды, которые его попросили (единажды).
   6. ssh
      sshd(8) и ssh -- программы, которые позволяют установить зашифрованное сообщение на незашифрованной сети
** 3 Стандартные файлы /etc: fstab, mtab, sysctl.conf, motd, issue, nologin.
   1. fstab -- file system table, файл который говорит, какие разделы куда нужно монтировать при init'е
   2. mtab -- mounted table, там написано что сейчас замонтировано и как
   3. sysctl.conf -- содержит настройки, которые необходимы sysctl для смены конфигурации ядра в рантайме
   4. motd -- (message of the day) все что там написано выводится после успешного login
   5. issue -- выводится до логина
   6. nologin -- если /etc/login существует, то логиниться можно только root'у
* 3 Ticket 16   Linking
** [Sources]
   1. Strange, complex
      http://flint.cs.yale.edu/cs422/doc/ELF_Format.pdf
   2. Simple, useful
      https://github.com/oxij/unix-notes-ru/blob/master/compiled/main.pdf
** 3 ELF
   Файлы мы умеем компилировать, линковать и запускать. Под компиляцией понимается превращение кода на чем-то в набор op-code'ов, находящихся в некотором формате. Tool chain -- последовательность действий необходимая для создания рабочего бинарника из кода.
   .h файлы представляют собой типы, методы, оффсеты и прочее. .c файлы хранят другие вещи и компилируются в бинарник, формат которого в подавляющем большинстве случаев ELF (есть еще COFF, но никто его не использует).
   ELF состоит из:
   1. ELF Header
      Содержит информацию про то, что это вообще такое, всякие версии, архитектуру и прочее.
   2. Program header
      | type | flags | offset in body | length in body | offset in memory<vm, ph> | length in memory |
      Оффсет в физической памяти игнорируется, если программа юзерспейсная. Мапить вообще в физическую память полезно если какие-то страницы закэшированы, а какие-то нет. Тип может быть разным, к примеру MEM значит что оно замаплено в память, INTERPRET это сегмент, где находится интерпретатор. DWARF значит дебаг, RPATH содержит перемнные окружения.
      Самый простой интерпретатор ELF парсит программные хедеры и загружает сегменты туда, куда указано (в свое новое адресное пространство).
   3. Body
      Sections and code are here
   4. Section table
      | labelname | type | <offset, length in body> |
      Большинство секций -- это просто функции, но есть специальные:
      | .init | инициализация                     |
      | .fini | конец инициализации               |
      | .ctor | конструкторы                      |
      | .dtor | деструкторы                       |
      | .data | section .data                     |
      | .text | section .text                     |
      | .sym  | таблица символов {name → address} |
      | .rel  | таблица релокаций                 |
      Поясним, что символом называется любая глобальная переменная, метка, функция и вообще все, у чего есть имя. Очевидно, что в .sym не может быть коллизий -- в программе нет функций с одинаковыми именами и т.д.

   Насчет аллокаций:
   По дефолту если мы вызываем какую-то внешнюю фукнцию из кода, то в объектнике появляется call 0x0, а в таблицу аллокаций добавляется {оффсет этого call, имя метки}.
   Структура таблицы релокаций:
   | name | type | offset |
   Что бы это могло значить -- хз, дальше понятнее станет.
** 3 Статическая линковка
   Когда ld линкует файлы, он:
   1. Берет кучу .o файлов, с зависимостями,
   2. Добавляет свои специальные объектники, чтобы сегенерировать _start и прочие жизненно важные вещи (crti.o, crtn.o,...).
   3. Игнорирует прогрмамные хедеры (выкидывает), мерджит объектники, склеивая секции в том порядке, в котором были даны аргументы,
   4. С каждым новым мерджем генерируeтся новый программный хедер.
   5. Резолвит таблицу релокаций, но пока не подставляет конкретные вызовы.

   Когда наступает время создать бинарник из .o, ld:
   1. проверяет наличие _start, другие вещи
   2. заполняет адреса из таблицы релокаций
   3. выкидывает section table

   То есть смотрит на все дырки, в которых ничего не написано (это хранится в .rel) и суммирует туда те адреса, которые значатся в таблице.
   Еще разок: на этапе компиляции в объектник были дырки и непонятно было, что вставлять, потом при мердже в таблицу добавились нужные записи.
   Линковка на уровне символов (symbol-level linking) это выкидывание ненужных символов на этапе создания бинарника. В Plan 9 это главная идея. С использованием линковки с выбрасыванием ненужного + статической можно легко имплементировать исполнение в контейнерах. Плюс забавно, что статически линковать выгоднее иногда, чем использовать динамическую линковку (пока мы не юзаем чего-нибудь ресурсоемкого типа каких-нибудь firefox).
** 3 Динамическая линковка, GOT, PLT
   Проблемы статической линковки: жирные бинарники, нужно перекомпилировать все подряд.
   Решение: на этапе мерджа мы мерджим то, что можно, а функции из внешних библиотек проверяем на целостность. При динамическом запуске динамический загрузчик кроме нашего кода загружает в память еще и библиотеку, причем расставляет правильные адреса вызовам.
   Есть проблема, что разложение объектников в памяти может портиться, поэтому непонятно, как расставлять адреса на этапе динамической загрузки. В x64 поддерживается relative адресация, а если такого нету, то можно использовать такой хак: Пусть в объектнике все адреса меток располагаются так, будто они начинаются с нуля, тогда при загрузке на адрес N можно ко всем вызовам прибавить N и будет работать. Код который можно так использовать, называют релокабельным.
   Насчет внешних вызовов, следует пользоватся таблицей релокации.
   Пусть у нас есть код, секция данных, и еще две секции -- таблица внешних вызовов и таблица внешних данных. Все call на внешние функции указывают на записи в таблице внешних вызовов, а там написано "jmp X", где X потом поставит компилятор. Аналогично с глобальными переменными, только вместо jump хранится просто адрес. То есть мы получили блоб кода с двумя табличками, которые нужно поменять и код будет работать. Такой код к тому же можно сделать релокабельным, то есть все внутренние метки имеют адрес как будто они нумеруются с нуля.
   Тогда делаем следующее:
   1. берем кучу объектников, клеим, клеим, клеим (то, что клеится хорошо, то есть не содержит вызовов куда-то туда, что мы не компилируем -- либы).
   2. получаем несколько бинарников
   3. раскладываем в память, добавляя ко всем вызовам внутри каждого бинарника адрес по которому он кладется (релокативность вот этого блоба) -- если требуется и у нас не relative раскладывание, оно в x64 из коробки
   4. динамический загрузчик расставляет адреса в 2 таблички каждого бинарника
   5. управление передается _start.

   На деле вместо таблицы внешних вызовов в таблицу внешних данных кладутся адреса функций. После чего ее называют GOT (Global offset table) -- для каждого бинарника на нее ссылаются все вызовы внутри него, а в самой табличке динамический линковщик проставляет адреса.
   С другой стороны, таблицу внешних вызовов выкидвать не нужно, а стоит поменять в каждом "jmp X" X на соответствующий элемент из GOT, а таблицу назовем PLT (procedure linkage table).
   Еще раз в виде определений:
   1. GOT -- таблица в которой лежат адреса всех внешних переменных и функций
   2. PLT -- таблица на которую ссылается код релокабельного бинарника, состоит из jmp на GOT
   Тогда динамическому линковщику нужно заполнять только GOT.
** 3 PIC
   Иногда хочется (из соображений безопасности) класть бинарник в случайное место памяти. Кроме того, затратно каждый раз прибавлять к каждой инструкции в коде адрес начала бинарника когда мы его куда-то кладем.
   PIC -- placement independent code -- код, исполнение которого не зависит от того, куда мы его положили.
   Вопрос в том, как сделать относительную адресацию на архитектуре, на которой это не работает по дефолту. В x64 есть RIP-addressing, который работает из коробки.
   Простое решение -- хранить ссылку на GOT в ebx, а все обращения к локальным переменным и регистрам проиходит через GOT.
** 2 Exec magic и интерпретаторы и прочее
   Shebang -- если файл начинается с #!, то это не ELF, а shebang формат, ядро запускает интерпретатор ровно оттуда, что указано после #!.
   Exec magic -- это вот это самое #!, которое 0x23 0x21. exec-функции умеют как раз парсить эту магию и понимать, что нужно делать с файлом, как искать интерпретатор и т.д.
   Насчет magic еще есть очень важное замечание, которое есть в magic(1). Вкратце, если я все правильно понимаю, /etc/share/misc/magic* файлы определяют, в каких местах в файлах могут находиться некоторые критичные данные, которые помогают типизировать его. Соответственно, сопоставляется MIME-тип. Утилита file прогоняет 3 теста (проеврка fs, проверка magic, проверка language). Насчет третьего вообще хз, первые два более-менее понятно.

   Кроме того, в Unix есть два динамических загрузчика -- один в ядре, который осиляет только простые ELF, другой юзерспейсный, очень сложный, но скомпилированный в простой ELF.
   Под динамическим загрузчиком понимают именно второй, он обычно лежит в /lib/ld-linux.so.VERSION, где VERSION -- его мажорная версия. По дефолту в переменной окружения LD_LIBRARY_PATH лежат библиотеки, которые ld осматривает при линковке.
* Uncategoriesed
** Execution levels
   There's -2 level of execution, for example something that governs how to operate with cooler.
   Available protocols for hard drives and stuff:
** Encryption and safety
   TPM -- hardware, that has 256-bit registers, near 20 items of them, has clear operation and extend reg data operation -- takes hash from data, hashes it with register and writes to it. TMP has some processor, that can clear, extend, and built-in algorithms of encryption, like AES (very secure, there's a proof, thats energy to decrypt it is more than the energy to melt the Earth: http://www.eetimes.com/document.asp?doc_id=1279619). There's also seal operation, that takes registers, data, and uses registers to encrypt data, then puts data into one of hardware box; There's also unseal operation, that does the opposite.
   Hardware encryption: LUKS, 2mb of data in the start of disk, that contains metadata, salt, master-key, header, algorithm for hashing passwords and disk, and then it's some magic. The disk is always encrypted. Hash cache is algorithm of detecting SPAM -- user that sends email generates hash collisions and it takes lot of computer time.
   That's not clearly secure (LUKS) because one can replace initrd. That's called evil maid strategy. We can encrypt all except grub, but there can be an malware in grub. There can be a solution with outer flash drive, that contains everything but encrypted binary blob (even LUKS header). With TPM it's easier, because things that BIOS does are extended. BIOS CAN HAVE MALWARE TOO!! PARANOID!! TXT is a technology by Intel that allows you to trust your processor, and it uses TMP to extend some hardware id.
** Read
   Что делает read:
   мы делаем read на файловый дескриптор.
   fdtable
   | int | file* |

   Первым делом read получает положение указателя в этом файле.
   Вызывается функция "верни мне указатель на функцию которая делает read для конкретной файловой системы".

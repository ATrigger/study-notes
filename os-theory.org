#+STARTUP: content
#+TITLE: Tickets for exam on operating systems lectures course given by Yan Malakhovski, 4th term, 2015

* Sources
  1. Raw notes by volhovm
  2. OS course plan
     http://rain.ifmo.ru/~trojan/linux/year2009/
  3. Yet another plan
     http://rain.ifmo.ru/~trojan/linux/year2007/
  4. Prev. year googledoc
     https://docs.google.com/spreadsheets/d/1CoPjN7shou3m3kAQdsRHY1HxQYyKho5f7Qn1KeL9n-U/edit#gid=0
  5. Malakhovski's notes
     https://github.com/oxij/unix-notes-ru/blob/master/compiled/main.pdf
  6. Wikipedia ofc
* Ticket 1
** RAM, SRAM, DRAM
   RAM (random-access memory) - это оперативная память.
   От постоянной памяти ее отличает:
   1) скорость работы
   2) потеря данных в отсутствие питания
   3) да вообще все по-другому!

   SRAM (static random-access memory) - статическая память.
   CMOS-ячейка SRAM основана на защелке: [[http://2.bp.blogspot.com/-dCCrTGB-c6U/T1zaY5TG1oI/AAAAAAAAAu8/MutoYbjglvs/s1600/SRAM.gif][схема]]
   *Read-операция*: мы даем на WL напряжение, сигнал уходит с BL и ¬BL.
   *Write-операция*: мы даем на WL напряжение и в BL тоже даем сигнал, он запоминается.

   Pros:
   + Быстрая
   + Не надо ничего перезаписывать
   Cons:
   - Защелки жрут электричество все время
   - Схема ячейки сложная, делать дороже
   - Ячейка занимает много площади

   DRAM (dynamic random-access memory) - динамическая память.
   CMOS-ячейка DRAM основана на конденсаторе: [[https://www.cs.auckland.ac.nz/~jmor159/363/html/fig/dram_cell.gif][схема]], [[https://upload.wikimedia.org/wikipedia/commons/3/3d/Square_array_of_mosfet_cells_read.png][схема банка]]
   *Read-операция*: мы даем на WL напряжение, конденсаторы разряжаются, сигнал уходит,
   и теперь его надо перезаписать обратно.
   *Write-операция*: даем на WL и BLs напряжения, те конденсаторы, на которых 1, заряжаются,
   а те, на которых 0 - разряжаются.
   *Замечание*: так как конденсаторы разряжаются сами по себе, периодически надо их перезаписывать
   (memory refresh)

   Pros:
   + Дешевая
   + Ячейки маленькие, можно много понапихать
   + Жрет энергию только во время read/write и refresh
   Cons:
   - Медленная (конденсаторы разряжаются не мгновенно)
   - Refresh-и тоже не прибавляют отзывчивости

** Структурно-операционная схема обычной планки памяти (DDR)
   Схема одного банка: [[https://upload.wikimedia.org/wikipedia/commons/3/3d/Square_array_of_mosfet_cells_read.png][схема банка]]
   Банки лежат на планочке рядом. Верхняя часть адреса ячейки отвечает за номер банка

   DDR (double-data rate) - хитрость, позволяющая передавать 2 слова за такт процессора вместо 1
   Хитрость в том, что мы передаем данные как на восходящем, так и на нисходящем фронте меандра.
** Кеши CPU: L1, L2, L3
   Кэш - быстрый кусок SRAM рядом с процессором, в который складываются часто используемые данные
   Кэш - всегда SRAM, чтобы было быстро, и потому всегда маленький, чтобы было не так
   дорого и энергозатратно.

   L1 - это самый маленький и близкий к процессору кэш. Он обычно сидит на том же куске
   кремния. Иногда подразделяется на L1i (кэш инструкций) b L1d (кэш данных)
   Его подстраховывает кэш L2 - который побольше и чуть подальше.
   Для многоядерных процессоров есть также кэш L3, к которому могут обращаться все ядра.
   (подробнее про это все - в последнем пункте)

   Кэши могут быть inclusive (L1 ⊂ L2 ⊂ L3 - данные дублируются) или exclusive (данные не дублируются)

   Кэш разбит на кэш-линии -- последовательно индексируемые куски памяти
   Структура кэш-линии:
   | tag | data block | flags |

   * tag - кусок адреса того куска памяти, который дублируется в этой кэш-линии.
     Нужен, чтобы искать данные по адресу в кэше
   * data - собственно, сами данные
   * flags - различная мета-инфа, а еще тут бывают коды коррекции ошибок

   Обычный адрес в памяти выглядит так:
   | tag | index | offset |
   * tag - как раз тот кусок, который матчится с tag в кэше
   * index - индекс кэш-линии, определяет, в каком наборе линий искать данные
   * offset - отступ от начала линии

   Контроллер обычной памяти же воспринимает адрес по-своему (как?)

   По методике синхронизации с памятью кэши бывают write-through и write-back.
   write-through - это когда любой запрос на запись всегда досылается в память,
   write-back - это когда данные из кэша дампятся в память только при вытеснении

   sources:
   [[https://en.wikipedia.org/wiki/CPU_cache]]
   [[http://arstechnica.com/gadgets/2002/07/caching/2/]]
** Схема параллельного извлечения
   Параллельное извлечение используется в кэше (там где у кэш-линий есть tag)
   Мы просто посылаем tag адреса на компараторы кэш-линий. Компаратор сравнивает
   tag линии с переданным, и высылает 1, если он совпал, и 0 - если нет.

   Данные извлекаются из всех кэш-линий одновременно, и данные каждой кэш-линии
   and-ятся с результатом компаратора. Выходные провода данных спаиваются вместе,
   и в итоге на выходе получаются только данные из кэш-линии с нужным тэгом.

   Если же все компараторы вернули 0, то мы детектируем cache miss и перенаправляем запрос к
   контроллеру памяти.

   sources:
   [[http://www.csbio.unc.edu/mcmillan/Media/L20Spring2012.pdf]]
   [[http://lwn.net/Articles/252125/]]
** Извлечение демультиплексором
   Демультиплексор - это штука, которая принимает n-битное число и сигнал
   и дает этот сигнал на соответствующий числу выход (один из 2ⁿ).
   [[https://en.wikipedia.org/wiki/Multiplexer#/media/File:Demultiplexer_Example01.svg][Схема]]

   Извлекать данные демультиплексором надо так:
   [[http://lwn.net/images/cpumemory/cpumemory.9.png][схема]]
   Row Address Selection - демультиплексор (выбираем строку но номеру)
   Column Address Selection - мультиплексор (выбираем столбец по номеру из тех, что пришли)

   Демультиплексор также используется в кэшах с неполной ассоциативностью, где с его
   помощью извлекаются кэш-линии с заданным индексом.
** Ассоциативность
   Суть кэша - в том, чтобы быстро проверить, лежат ли данные по
   заданному адресу в кэше, и если да - вернуть их.

   Если кэш полностью ассоциативный (fully-associative), то мы должны сделать параллельное
   извлечение сразу из всего кэша! С этим есть несколько бед:
   - Теги должны быть большими, слишком много места в кэше отведено под тэги
   - Теги большие - и компараторы для них тоже большие, а значит -
     дорогие и медленные

   Однако, есть плюсы:
   + Не бывает коллизий кэша - если мы хотим положить что-то в кэш и в нем еще
     вообще есть место, нам это удастся
   + Как следствие этого, происходит мало cache miss-ов

   Противоположность: direct-mapped cache.
   Адрес в памяти однозначно определяет кэш-линию,
   в которой эта ячейка памяти может содержаться.

   Делается это так: адрес демультиплексируется по index, выбирается 1 кэш-линия,
   тэг в ней сравнивается с данным, и если тэг равен - данные кэш-линии возвращаются.
   Иначе - регистрируется кэш-мисс.

   Pros:
   + Все это делается быстро, нет кучи компараторов
   Cons:
   - Так как каждая ячейка памяти может быть сохранена только в 1 кэш-линии,
     возникает множество *коллизий* - это когда 2 разные ячейки попадают в одну
     и ту же кэш-линию и кто-то из них вытесняет другую
   - Вследствие этого, много кэш-миссов

   Золотая середина - n-ассоциативный кэш, когда каждая ячейка может содержаться в
   одной из n линий. Делается это так:
   1) Демультиплексор отсеивает n кэш-линий по index адреса
   2) Из этих n линий по тэгу извлекаются параллельно данные

   sources:
   [[http://www.csbio.unc.edu/mcmillan/Media/L20Spring2012.pdf]]
   [[http://arstechnica.com/gadgets/2002/07/caching/5/]]
   [[http://arstechnica.com/gadgets/2002/07/caching/6/]]
** TLB
   TLB - translation lookaside buffer - это такой специальный кэш, который
   маппит виртуальные адреса в реальные.
   Он небольшой, и, конечно, не содержит все используемые виртуальные адреса, а лишь
   часто используемые.
   Трансляцией из виртуальных адресов в реальные занимается MMU - memory management unit
   - специальный кусок процессора.
   Он глядит в TLB, и если не находит там, пускается в долгий путь
   по page table-ам в основной памяти
** Общее влияние кеша на работу с памятью
   Кэш, в целом, ускоряет работу с памятью (кто бы мог подумать?)
   Иногда получается так, что работа идет только с кэшом, а к памяти обращений
   и вовсе нет (в случае look-aside)

   ?? Что здесь написать ??
** Кеши в мультипроцессорных системах и когерентность кешей
   Если у нас есть много ядер, то у каждого ядра есть собственные
   кэши L1 и L2.
   Однако, что же делать, если одна и та же ячейка памяти продублирована
   в кэшах разных ядер, и одно ядро меняет эту ячейку в своем кэше?
   Другое должно как-то увидеть это изменение.

   Для таких ситуаций существует *протоколы когерентности кэша*
   Например, MESI

   Во flags каждой кэш-линии кодируется ее состояние, 1 из 4:
   * Modified  - актуальная кэш-линия есть только в этом кэше, и она была изменена,
     то есть не соответствует данным в основной памяти
   * Exclusive - кэш-линия актуальна только в этом кэше, и она совпадает с данными в памяти.
   * Shared    - кэш-линия совпадает с данными в памяти и может присутствовать в нескольких кэшах
   * Invalid   - кэш-линия невалидна

   Read может происходить из любого состояния, кроме Invalid. Если пытаемся читать
   из Invalid, то нужно сначала пофетчить данные (извлечь из соседних кэшей или из памяти).

   Write может происходить только в Modified или Exclusive. Иначе сначала нам нужно инвалидировать
   все копии в других кэшах, а потом выставить статус Modified.

   Перед инвалидацией Modified-линии нужно сбросить данные из нее в память (write-back)

   sources:
   [[https://en.wikipedia.org/wiki/MESI_protocol]]
* Ticket 2
** Пайплайн и стадии
** Регистровый файл
** Пузыри (pipeline bubbles)
** Предсказание переходов (branch prediction)
** Out of order исполнение
** Интерфейс между устройствами ввода и CPU
** Прерывания
** DMA
* Ticket 3
** Виртуальная память
** MMU: TLB, каталог страниц (page table)
** Биты: readable, writeable, executable, present, dirty, copy-on-write
** IOMMU
** Память процессов
** Общая память
** Пулы памяти со специальными требованиями.
* Ticket 4
** "География" адресного пространства процесса
** Структуры ядра описывающие процесс с MMU: работа с физической памятью, VMA
** Системные вызовы: brk, sbrk, mmap
** Общая память: mmaping файлов и shm_open
** Реализация malloc.
* Ticket 5
** Процессы и треды
** init, родители, дети, зомби
** Треды, группы тредов, процессы, группы процессов, сессии
** Системные вызовы fork, clone, exec, wait
** Интерфейс bash
** Реализация переключения контекстов процессов: структуры данных ядра, состояния процессов, различные методы реализации CPS-преобразования.
* Ticket 6
** Файловые дескрипторы и пайпы
** Системные вызовы open, read, write, close
** Структуры данных ядра: таблица файловых дескрипторов, файловые объекты POSIX, флаг CLOEXEC
** Системные вызовы dup2, fcntl, flock
** Структуры данных ядра для реализации пайпов, семафоров, блокировок на файлы.
* Ticket 7
** Драйвера устройств в пространстве ядра
** Прерывания
** Монолитная обработка прерываний
** Hi/Lo прерывания
** Polling
** Драйвера как контексты исполнения и их отличия от процессов
** Реализация драйверов: структуры данных ядра, различные методы реализации CPS-преобразования.
* Ticket 8
** Файловые системы
** Структура данных inode
** VFS
** Структуры данных: FSObject, Namespace
** Path resolution
** Операции над неймспейсами: mount, bind mount, move mount, chroot, pivot_root
** Linux FUSE
** ФС как функция inodeno → inode
** Структуры ядра: файловый дескриптор (на устройство, файл, директорию), различные кеши
** mmaping файлов.
* Ticket 9
** Пользователи и права
** Модели прав доступа к объектам: дискретная и ролевая
** Права на объекты файловой системы
** Пользователи и группы с точки зрения ядра
** Пользователи и группы с точки зрения пространства пользователя
** Системные вызовы setuid, setgid и товарищи
** setuid bit
** PAM
** /etc/passwd, /etc/shadow, /etc/group
** Capabilities.
* Ticket 10
** Стандартные сигналы
** Маски
** Правила доставки
** Реалтаймовые сигналы
** Маски и очереди
** Правила доставки
** Системные вызовы kill, sigaction
** Прерывание сигналами: кода программы, обработчиков сигналов, системных вызовов
** Реентрабельность и безопасные системные вызовы
** Сигналы и треды
** Семантика сигналов: TERM, KILL, STOP, CONT, CHLD, PIPE, ILL/FPE, SEGV, BUS.
* Ticket 11
** Мултиплексирование ввода-вывода
** O_NONBLOCK
** Edge и level triggered события
** Преобразование асинхронного ввода-вывода в синхронный CPS-преобразованием
** Структуры данных пространства ядра для реализации мультиплексора файловых дескрипторов
** Системные вызовы select, poll, epoll
** Управление скоростью передачи данных через файловые дескрипторы.
* Ticket 12
** Синхронизация
** Спинлоки
** Ядерные семафоры
** Блокировки и лизинги на файлы
** Структуры данных пространства ядра для реализации блокировок и лизингов
** Системные вызовы: flock, fcntl.
* Ticket 13
** Сетевой стек
** PPP, Ethernet
** IP
** TCP, UDP, SCTP
** BSD sockets: API, Stream-сокеты, Datagram-сокеты, RAW-сокеты, файловый объект для accept-сокета.
* Ticket 14
** Терминалы и управление заданиями в POSIX
** Терминалы, псевдотерминалы и режимы их работы
** Группы процессов и сессии
** Foreground и background группы
** Сигналы: INT, HUP, TSTP, TTIN, TTOU, WINCH
** Демоны и демонизация.
* Ticket 15
** Загрузка: BIOS → MBR (DOS Label), DOS/Windows boot, GRUB
** initrd
** Инициализация системы: последовательная, учитывая зависимости, resource/socket activation, lazy activation
** Стандартные init системы: System V init (не забыв про runlevel), Upstart, OpenRC, systemd
** Стандартные демоны: init, syslog, klog, cron, at, ssh
** Стандартные файлы /etc: fstab, mtab, sysctl.conf, motd, issue, nologin.
* Ticket 16
** Запуск программ, динамическая линковка и загрузка
** Exec magic и интерпретаторы
** Релокация кода: релокационные дырки, кеширование релокаций, GOT, PIC
** Объектные, исполняемые и библиотечные файлы
** Формат ELF
** ld-linux и его x86 32 шные ужасы.
